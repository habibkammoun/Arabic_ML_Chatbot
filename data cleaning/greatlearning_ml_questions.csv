question,answer
"Explain the terms Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning?","Artificial Intelligence (AI) is the domain of producing intelligent machines. ML refers to systems that can assimilate from experience (training data) and Deep Learning (DL) states to systems that learn from experience on large data sets. ML can be considered as a subset of AI. Deep Learning (DL) is ML but useful to large data sets. The figure below roughly encapsulates the relation between AI, ML, and DL:"
What are the different types of Learning/ Training models in ML?,ML algorithms can be primarily classified depending on the presence/absence of target variables.
What is the main key difference between supervised and unsupervised machine learning?,There are various means to select important variables from a data set that include the following:
"There are many machine learning algorithms till now. If given a data set, how can one determine which algorithm to be used for that?","Machine Learning algorithm to be used purely depends on the type of data in a given dataset. If data is linear then, we use linear regression. If data shows non-linearity then, the bagging algorithm would do better. If the data is to be analyzed/interpreted for some business purposes then we can use decision trees or SVM. If the dataset consists of images, videos, audios then, neural networks would be helpful to get the solution accurately."
State the differences between causality and correlation?,"Causality applies to situations where one action, say X, causes an outcome, say Y, whereas Correlation is just relating one action (X) to another action(Y) but X does not necessarily cause Y."
We look at machine learning software almost all the time. How do we apply Machine Learning to Hardware?,We have to build ML algorithms in System Verilog which is a Hardware development Language and then program it onto an FPGA to apply Machine Learning to hardware.
Explain One-hot encoding and Label Encoding. How do they affect the dimensionality of the given dataset?,"One-hot encoding is the representation of categorical variables as binary vectors. Label Encoding is converting labels/words into numeric form. Using one-hot encoding increases the dimensionality of the data set. Label encoding doesn’t affect the dimensionality of the data set. One-hot encoding creates a new variable for each level in the variable whereas, in Label encoding, the levels of a variable get encoded as 1 and 0."
"What is Bias, Variance and what do you mean by Bias-Variance Tradeoff?","Both are errors in Machine Learning Algorithms. When the algorithm has limited flexibility to deduce the correct observation from the dataset, it results in bias. On the other hand, variance occurs when the model is extremely sensitive to small fluctuations."
How can we relate standard deviation and variance?,Standard deviation refers to the spread of your data from the mean. Variance is the average degree to which each point differs from the mean i.e. the average of all data points. We can relate Standard deviation and Variance because it is the square root of Variance.
A data set is given to you and it has missing values which spread along 1 standard deviation from the mean. How much of the data would remain untouched?,"It is given that the data is spread across mean that is the data is spread across an average. So, we can presume that it is a normal distribution. In a normal distribution, about 68% of data lies in 1 standard deviation from averages like mean, mode or median. That means about 32% of the data remains uninfluenced by missing values."
Is a high variance in data good or bad?,"Higher variance directly means that the data spread is big and the feature has a variety of data. Usually, high variance in a feature is seen as not so good quality."
"If your dataset is suffering from high variance, how would you handle it?","For datasets with high variance, we could use the bagging algorithm to handle it. Bagging algorithm splits the data into subgroups with sampling replicated from random data. After the data is split, random data is used to create rules using a training algorithm. Then we use polling technique to combine all the predicted outcomes of the model."
"A data set is given to you about utilities fraud detection. You have built aclassifier model and achieved a performance score of 98.5%. Is this a goodmodel? If yes, justify. If not, what can you do about it?","Data set about utilities fraud detection is not balanced enough i.e. imbalanced. In such a data set, accuracy score cannot be the measure of performance as it may only be predict the majority class label correctly but in this case our point of interest is to predict the minority label. But often minorities are treated as noise and ignored. So, there is a high probability of misclassification of the minority label as compared to the majority label. For evaluating the model performance in case of imbalanced data sets, we should use Sensitivity (True Positive rate) or Specificity (True Negative rate) to determine class label wise performance of the classification model. If the minority class label’s performance is not so good, we could do the following:"
Explain the handling of missing or corrupted values in the given dataset.,An easy way to handle missing values or corrupted values is to drop the corresponding rows or columns. If there are too many rows or columns to drop then we consider replacing the missing or corrupted values with some new value.
What is Time series?,"A Time series is a sequence of numerical data points in successive order. It tracks the movement of the chosen data points, over a specified period of time and records the data points at regular intervals. Time series doesn’t require any minimum or maximum time input. Analysts often use Time series to examine data according to their specific requirement."
What is a Box-Cox transformation?,Box-Cox transformation is a power transform which transforms non-normal dependent variables into normal variables as normality is the most common assumption made while using many statistical techniques. It has a lambda parameter which when set to 0 implies that this transform is equivalent to log-transform. It is used for variance stabilization and also to normalize the distribution.
What is the exploding gradient problem while using the back propagation technique?,"When large error gradients accumulate and result in large changes in the neural network weights during training, it is called the exploding gradient problem. The values of weights can become so large as to overflow and result in NaN values. This makes the model unstable and the learning of the model to stall just like the vanishing gradient problem. This is one of the most commonly asked interview questions on machine learning."
Explain the differences between Random Forest and Gradient Boosting machines.,Confusion matrix (also called the error matrix) is a table that is frequently used to illustrate the performance of a classification model i.e. classifier on a set of test data for which the true values are well-known.
What is a confusion matrix and why do you need it?,Confusion matrix (also called the error matrix) is a table that is frequently used to illustrate the performance of a classification model i.e. classifier on a set of test data for which the true values are well-known.
What’s a Fourier transform?,"Fourier Transform is a mathematical technique that transforms any function of time to a function of frequency. Fourier transform is closely related to Fourier series. It takes any time-based pattern for input and calculates the overall cycle offset, rotation speed and strength for all possible cycles. Fourier transform is best applied to waveforms since it has functions of time and space. Once a Fourier transform applied on a waveform, it gets decomposed into a sinusoid."
What do you mean by Associative Rule Mining (ARM)?,Associative Rule Mining is one of the techniques to discover patterns in data like features (dimensions) which occur together and features (dimensions) which are correlated. It is mostly used in Market-based Analysis to find how frequently an itemset occurs in a transaction. Association rules have to satisfy minimum support and minimum confidence at the very same time. Association rule generation generally comprised of two different steps:
What is Marginalisation? Explain the process.,Marginalisation is summing the probability of a random variable X given joint probability distribution of X with other variables. It is an application of the law of total probability.
Explain the phrase “Curse of Dimensionality”.,The Curse of Dimensionality refers to the situation when your data has too many features.
Why is rotation of components so important in Principle Component Analysis (PCA)?,"Rotation in PCA is very important as it maximizes the separation within the variance obtained by all the components because of which interpretation of components would become easier. If the components are not rotated, then we need extended components to describe variance of the components."
Explain the difference between Normalization and Standardization.,Normalization and Standardization are the two very popular methods used for feature scaling.
What is Linear Regression?,"Linear Function can be defined as a Mathematical function on a 2D plane as, Y =Mx +C, where Y is a dependent variable and X is Independent Variable, C is Intercept and M is slope and same can be expressed as Y is a Function of X or Y = F(x)."
What could be the issue when the beta value for a certain variable varies way too much in each subset when regression is run on different subsets of the given dataset?,"Variations in the beta values in every subset implies that the dataset is heterogeneous. To overcome this problem, we can use a different model for each of the dataset’s clustered subsets or a non-parametric model such as decision trees."
What does the term Variance Inflation Factor mean?,Variation Inflation Factor (VIF) is the ratio of the model’s variance to the model’s variance with only one independent variable. VIF gives the estimate of the volume of multicollinearity in a set of many regression variables.
What is Kernel Trick in an SVM Algorithm?,"Kernel Trick is a mathematical function which when applied on data points, can find the region of classification between two different classes. Based on the choice of function, be it linear or radial, which purely depends upon the distribution of data, one can build a classifier."
How do you handle outliers in the data?,"Outlier is an observation in the data set that is far away from other observations in the data set. We can discover outliers using tools and functions like box plot, scatter plot, Z-Score, IQR score etc. and then handle them based on the visualization we have got. To handle outliers, we can cap at some threshold, use transformations to reduce skewness of the data and remove outliers if they are anomalies or errors."
State the limitations of Fixed Basis Function.,"Linear separability in feature space doesn’t imply linear separability in input space. So, Inputs are non-linearly transformed using vectors of basic functions with increased dimensionality. Limitations of Fixed basis functions are:"
Define and explain the concept of Inductive Bias with some examples.,"Inductive Bias is a set of assumptions that humans use to predict outputs given inputs that the learning algorithm has not encountered yet. When we are trying to learn Y from X and the hypothesis space for Y is infinite, we need to reduce the scope by our beliefs/assumptions about the hypothesis space which is also called inductive bias. Through these assumptions, we constrain our hypothesis space and also get the capability to incrementally test and improve on the data using hyper-parameters. Examples:"
Explain the term instance-based learning.,Instance Based Learning is a set of procedures for regression and classification which produce a class label prediction based on resemblance to its nearest neighbors in the training data set. These algorithms just collects all the data and get an answer when required or queried. In simple words they are a set of procedures for solving new problems based on the solutions of already solved problems in the past which are similar to the current problem.
"Define precision, recall and F1 Score?",The metric used to access the performance of the classification model is Confusion Metric. Confusion Metric can be further interpreted with the following terms:-
Plot validation score and training score with data set size on the x-axis and another plot with model complexity on the x-axis.,"For high bias in the models, the performance of the model on the validation data set is similar to the performance on the training data set. For high variance in the models, the performance of the model on the validation set is worse than the performance on the training set."
What is Bayes’ Theorem? State at least 1 use case with respect to the machine learning context?,"Bayes’ Theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if cancer is related to age, then, using Bayes’ theorem, a person’s age can be used to more accurately assess the probability that they have cancer than can be done without the knowledge of the person’s age.Chain rule for Bayesian probability can be used to predict the likelihood of the next word in the sentence."
What is Naive Bayes? Why is it Naive?,Naive Bayes classifiers are a series of classification algorithms that are based on the Bayes theorem. This family of algorithm shares a common principle which treats every pair of features independently while being classified.
Explain how a Naive Bayes Classifier works.,Naive Bayes classifiers are a family of algorithms which are derived from the Bayes theorem of probability. It works on the fundamental assumption that every set of two features that is being classified is independent of each other and every feature makes an equal and independent contribution to the outcome.
What do the terms prior probability and marginal likelihood in context of Naive Bayes theorem mean?,"Prior probability is the percentage of dependent binary variables in the data set. If you are given a dataset and dependent variable is either 1 or 0 and percentage of 1 is 65% and percentage of 0 is 35%. Then, the probability that any new input for that variable of being 1 would be 65%."
Explain the difference between Lasso and Ridge?,"Lasso(L1) and Ridge(L2) are the regularization techniques where we penalize the coefficients to find the optimum solution. In ridge, the penalty function is defined by the sum of the squares of the coefficients and for the Lasso, we penalize the sum of the absolute values of the coefficients. Another type of regularization method is ElasticNet, it is a hybrid penalizing function of both lasso and ridge."
What’s the difference between probability and likelihood?,"Probability is the measure of the likelihood that an event will occur that is, what is the certainty that a specific event will occur? Where-as a likelihood function is a function of parameters within the parameter space that describes the probability of obtaining the observed data.So the fundamental difference is, Probability attaches to possible results; likelihood attaches to hypotheses."
Why would you Prune your tree?,"In the context of data science or AIML, pruning refers to the process of reducing redundant branches of a decision tree. Decision Trees are prone to overfitting, pruning the tree helps to reduce the size and minimizes the chances of overfitting. Pruning involves turning branches of a decision tree into leaf nodes and removing the leaf nodes from the original branch. It serves as a tool to perform the tradeoff."
Model accuracy or Model performance? Which one will you prefer and why?,"This is a trick question, one should first get a clear idea, what is Model Performance? If Performance means speed, then it depends upon the nature of the application, any application related to the real-time scenario will need high speed as an important feature. Example: The best of Search Results will lose its virtue if the Query results do not appear fast."
List the advantages and limitations of the Temporal Difference Learning Method.,Temporal Difference Learning Method is a mix of Monte Carlo method and Dynamic programming method. Some of the advantages of this method include:
How would you handle an imbalanced dataset?,"Sampling Techniques can help with an imbalanced dataset. There are two ways to perform sampling, Under Sample or Over Sampling."
Mention some of the EDA Techniques?,Exploratory Data Analysis (EDA) helps analysts to understand the data better and forms the foundation of better models.
Mention why feature engineering is important in model building and list out some of the techniques used for feature engineering.,"Algorithms necessitate features with some specific characteristics to work appropriately. The data is initially in a raw form. You need to extract features from this data before supplying it to the algorithm. This process is called feature engineering. When you have relevant features, the complexity of the algorithms reduces. Then, even if a non-ideal algorithm is used, results come out to be accurate."
Differentiate between Statistical Modeling and Machine Learning?,"Machine learning models are about making accurate predictions about the situations, like Foot Fall in restaurants, Stock-Price, etc. where-as, Statistical models are designed for inference about the relationships between variables, as What drives the sales in a restaurant, is it food or Ambience."
Differentiate between Boosting and Bagging?,Bagging and Boosting are variants of Ensemble Techniques.
What is the significance of Gamma and Regularization in SVM?,"The gamma defines influence. Low values meaning ‘far’ and high values meaning ‘close’. If gamma is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with C will be able to prevent overfitting. If gamma is very small, the model is too constrained and cannot capture the complexity of the data."
Define ROC curve work,The graphical representation of the contrast between true positive rates and the false positive rate at various thresholds is known as the ROC curve. It is used as a proxy for the trade-off between true positives vs the false positives.
What is the difference between a generative and discriminative model?,"A generative model learns the different categories of data. On the other hand, a discriminative model will only learn the distinctions between different categories of data. Discriminative models perform much better than the generative models when it comes to classification tasks."
What are hyperparameters and how are they different from parameters?,"A parameter is a variable that is internal to the model and whose value is estimated from the training data. They are often saved as part of the learned model. Examples include weights, biases etc."
What is shattering a set of points? Explain VC dimension.,"In order to shatter a given configuration of points, a classifier must be able to, for all possible assignments of positive and negative for the points, perfectly partition the plane such that positive points are separated from negative points. For a configuration of n points, there are 2n possible assignments of positive or negative."
What are some differences between a linked list and an array?,"Arrays and Linked lists are both used to store linear data of similar types. However, there are a few difference between them."
What is the meshgrid () method and the contourf () method? State some usesof both.,"The meshgrid( ) function in numpy takes two arguments as input : range of x-values in the grid, range of y-values in the grid whereas meshgrid needs to be built before the contourf( ) function in matplotlib is used which takes in many inputs : x-values, y-values, fitting curve (contour line) to be plotted in grid, colours etc."
Describe a hash table.,Hashing is a technique for identifying unique objects from a group of similar objects. Hash functions are large keys converted into small keys in hashing techniques. The values of hash functions are stored in data structures which are known hash table.
List the advantages and disadvantages of using Neural Networks.,Advantages:
You have to train a 12GB dataset using a neural network with a machine which has only 3GB RAM. How would you go about it?,"We can use NumPy arrays to solve this issue. Load all the data into an array. In NumPy, arrays have a property to map the complete dataset without loading it completely in memory. We can pass the index of the array, dividing data into batches, to get the data required and then pass the data into the neural networks. But be careful about keeping the batch size normal."
Write a simple code to binarize data.,Conversion of data into binary values on the basis of certain threshold is known as binarizing of data. Values below the threshold are set to 0 and those above the threshold are set to 1 which is useful for feature engineering.
What is an Array?,"The array is defined as a collection of similar items, stored in a contiguous manner. Arrays is an intuitive concept as the need to group similar objects together arises in our day to day lives. Arrays satisfy the same need. How are they stored in the memory? Arrays consume blocks of data, where each element in the array consumes one unit of memory. The size of the unit depends on the type of data being used. For example, if the data type of elements of the array is int, then 4 bytes of data will be used to store each element. For character data type, 1 byte will be used. This is implementation specific, and the above units may change from computer to computer."
What are the advantages and disadvantages of using an Array?,"Now that we know what arrays are, we shall understand them in detail by solving some interview questions. Before that, let us see the functions that Python as a language provides for arrays, also known as, lists."
What is Lists in Python?,"Lists is an effective data structure provided in python. There are various functionalities associated with the same. Let us consider the scenario where we want to copy a list to another list. If the same operation had to be done in C programming language, we would have to write our own function to implement the same."
"Given an array of integers where each element represents the max number of steps that can be made forward from that element. The task is to find the minimum number of jumps to reach the end of the array (starting from the first element). If an element is 0, then cannot move through that element.","Solution: This problem is famously called as end of array problem. We want to determine the minimum number of jumps required in order to reach the end. The element in the array represents the maximum number of jumps that, that particular element can take."
"Given a string S consisting only ‘a’s and ‘b’s, print the last index of the ‘b’ present in it.","When we have are given a string of a’s and b’s, we can immediately find out the first location of a character occurring. Therefore, to find the last occurrence of a character, we reverse the string and find the first occurrence, which is equivalent to the last occurrence in the original string."
Rotate the elements of an array by d positions to the left.,"There exists a pattern here, that is, the first d elements are being interchanged with last n-d +1 elements. Therefore we can just swap the elements. Correct? What if the size of the array is huge, say 10000 elements. There are chances of memory error, run-time error etc. Therefore, we do it more carefully. We rotate the elements one by one in order to prevent the above errors, in case of large arrays."
Water Trapping Problem,"Given an array arr[] of N non-negative integers which represents the height of blocks at index I, where the width of each block is 1. Compute how much water can be trapped in between blocks after raining."
Explain Eigenvectors and Eigenvalues.,Ans. Linear transformations are helpful to understand using eigenvectors. They find their prime usage in the creation of covariance and correlation matrices in data science.
102.,"Ans. The number of clusters can be determined by finding the silhouette score. Often we aim to get some inferences from data using clustering techniques so that we can have a broader picture of a number of classes being represented by the data. In this case, the silhouette score helps us determine the number of cluster centres to cluster our data along."
What are the performance metrics that can be used to estimate the efficiency of a linear regression model?,Ans. The performance metric that is used in this case is:
What is the default method of splitting in decision trees?,The default method of splitting in decision trees is the Gini Index. Gini Index is the measure of impurity of a particular node.
How is p-value useful?,"Ans. The p-value gives the probability of the null hypothesis is true. It gives us the statistical significance of our results. In other words, p-value determines the confidence of a model in a particular output."
Can logistic regression be used for classes more than 2?,"Ans. No, logistic regression cannot be used for classes more than 2 as it is a binary classifier. For multi-class classification algorithms like Decision Trees, Naïve Bayes’ Classifiers are better suited."
What are the hyperparameters of a logistic regression model?,"Ans. Classifier penalty, classifier solver and classifier C are the trainable hyperparameters of a Logistic Regression Classifier. These can be specified exclusively with values in Grid Search to hyper tune a Logistic Classifier."
Name a few hyper-parameters of decision trees?,Ans. The most important features which one can tune in decision trees are:
How to deal with multicollinearity?,Ans. Multi collinearity can be dealt with by the following steps:
What is,Ans. It is a situation in which the variance of a variable is unequal across the range of values of the predictor variable.
Is ARIMA model a good fit for every time series problem?,"Ans. No, ARIMA model is not suitable for every type of time series problem. There are situations where ARMA model and others also come in handy."
How do you deal with the class imbalance in a classification problem?,Ans. Class imbalance can be dealt with in the following ways:
What is the role of cross-validation?,"Ans. Cross-validation is a technique which is used to increase the performance of a machine learning algorithm, where the machine is fed sampled data out of the same data for a few times. The sampling is done so that the dataset is broken into small parts of the equal number of rows, and a random part is chosen as the test set, while all other parts are chosen as train sets."
What is a voting model?,"Ans. A voting model is an ensemble model which combines several classifiers but to produce the final result, in case of a classification-based model, takes into account, the classification of a certain data point of all the models and picks the most vouched/voted/generated option from all the given classes in the target column."
How to deal with very few data samples? Is it possible to make a model out of it?,"Ans. If very few data samples are there, we can make use of oversampling to produce new data points. In this way, we can have new data points."
What are the hyperparameters of an SVM?,"Ans. The gamma value, c value and the type of kernel are the hyperparameters of an SVM model."
What is Pandas Profiling?,Ans. Pandas profiling is a step to find the effective number of usable data. It gives us the statistics of NULL values and the usable values and thus makes variable selection and data selection for building models in the preprocessing phase very effective.
What impact does correlation have on PCA?,Ans. If data is correlated PCA does not work well. Because of the correlation of variables the effective variance of variables decreases. Hence correlated data when used for PCA does not work well.
How is PCA different from LDA?,Ans. PCA is unsupervised. LDA is unsupervised.
What distance metrics can be used in KNN?,Ans. Following distance metrics can be used in KNN.
Which metrics can be used to measure correlation of categorical data?,Ans. Chi square test can be used for doing so. It gives the measure of correlation between categorical predictors.
Which algorithm can be used in value imputation in both categorical and continuous categories of data?,Ans. KNN is the only algorithm that can be used for imputation of both categorical and continuous variables.
When should ridge regression be preferred over lasso?,Ans. We should use ridge regression when we want to use all predictors and not remove any as it reduces the coefficient values but does not nullify them.
Which algorithms can be used for important variable selection?,"Ans. Random Forest, Xgboost and plot variable importance charts can be used for variable selection."
What ensemble technique is used by Random forests?,Ans. Bagging is the technique used by Random Forests. Random forests are a collection of trees which work on sampled data from the original dataset with the final prediction being a voted average of all trees.
What ensemble technique is used by gradient boosting trees?,Ans. Boosting is the technique used by GBM.
If we have a high bias error what does it mean? How to treat it?,Ans. High bias error means that that model we are using is ignoring all the important trends in the model and the model is underfitting.
Which type of sampling is better for a classification model and why?,"Ans. Stratified sampling is better in case of classification problems because it takes into account the balance of classes in train and test sets. The proportion of classes is maintained and hence the model performs better. In case of random sampling of data, the data is divided into two parts without taking into consideration the balance classes in the train and test sets. Hence some classes might be present only in tarin sets or validation sets. Hence the results of the resulting model are poor in this case."
What is a good metric for measuring the level of multicollinearity?,"Ans. VIF or 1/tolerance is a good measure of measuring multicollinearity in models. VIF is the percentage of the variance of a predictor which remains unaffected by other predictors. So higher the VIF value, greater is the multicollinearity amongst the predictors."
When can be a categorical value treated as a continuous variable and what effect does it have when done so?,Ans. A categorical predictor can be treated as a continuous one when the nature of data points it represents is ordinal. If the predictor variable is having ordinal data then it can be treated as continuous and its inclusion in the model increases the performance of the model.
What is the role of maximum likelihood in logistic regression.,Ans. Maximum likelihood equation helps in estimation of most probable values of the estimator’s predictor variable coefficients which produces results which are the most likely or most probable and are quite close to the truth values.
Which distance do we measure in the case of KNN?,Ans. The hamming distance is measured in case of KNN for the determination of nearest neighbours. Kmeans uses euclidean distance.
What is a pipeline?,Ans. A pipeline is a sophisticated way of writing software such that each intended action while building a model can be serialized and the process calls the individual functions for the individual tasks. The tasks are carried out in sequence for a given sequence of data points and the entire process can be run onto n threads by use of composite estimators in scikit learn.
Which sampling technique is most suitable when working with time-series data?,Ans. We can use a custom iterative sampling such that we continuously add samples to the train set. We only should keep in mind that the sample used for validation should be added to the next train sets and a new sample is used for validation.
What are the benefits of pruning?,Ans. Pruning helps in the following:
What is normal distribution?,Ans. The distribution having the below properties is called normal distribution.
What is the 68 per cent rule in normal distribution?,Ans. The normal distribution is a bell-shaped curve. Most of the data points are around the median. Hence approximately 68 per cent of the data is around the median. Since there is no skewness and its bell-shaped.
What is a chi-square test?,Ans. A chi-square determines if a sample data matches a population.
What is a random variable,Ans. A Random Variable is a set of possible values from a random experiment. Example: Tossing a coin: we could get Heads or Tails. Rolling of a dice: we get 6 values
What is the degree of freedom?,Ans. It is the number of independent values or quantities which can be assigned to a statistical distribution. It is used in Hypothesis testing and chi-square test.
Which kind of recommendation system is used by amazon to recommend similar items?,Ans. Amazon uses a collaborative filtering algorithm for the recommendation of similar items. It’s a user to user similarity based mapping of user likeness and susceptibility to buy.
What is a false positive?,Ans. It is a test result which wrongly indicates that a particular condition or attribute is present.
What is a false negative?,Ans. A test result which wrongly indicates that a particular condition or attribute is absent.
What is the error term composed of in regression?,Ans. Error is a sum of bias error+variance error+ irreducible error in regression. Bias and variance error can be reduced but not the irreducible error.
Which performance metric is better R2 or adjusted R2?,Ans. Adjusted R2 because the performance of predictors impacts it. R2 is independent of predictors and shows performance improvement through increase if the number of predictors is increased.
What’s the difference between Type I and Type II error?,"Type I and Type II error in machine learning refers to false values. Type I is equivalent to a False positive while Type II is equivalent to a False negative. In Type I error, a hypothesis which ought to be accepted doesn’t get accepted. Similarly, for Type II error, the hypothesis gets rejected which should have been accepted in the first place."
What do you understand by L1 and L2 regularization?,L2 regularization: It tries to spread error among all the terms. L2 corresponds to a Gaussian prior.
"Which one is better, Naive Bayes Algorithm or Decision Trees?","Although it depends on the problem you are solving, but some general advantages are following:"
What do you mean by the ROC curve?,"Receiver operating characteristics (ROC curve): ROC curve illustrates the diagnostic ability of a binary classifier. It is calculated/created by plotting True Positive against False Positive at various threshold settings. The performance metric of ROC curve is AUC (area under curve). Higher the area under the curve, better the prediction power of the model."
What do you mean by AUC curve?,"AUC (area under curve). Higher the area under the curve, better the prediction power of the model."
What is log likelihood in logistic regression?,"It is the sum of the likelihood residuals. At record level, the natural log of the error (residual) is calculated for each record, multiplied by minus one, and those values are totaled. That total is then used as the basis for deviance (2 x ll) and likelihood (exp(ll))."
How would you evaluate a logistic regression model?,"Model Evaluation is a very important part in any analysis to answer the following questions,"
What are the advantages of SVM algorithms?,SVM algorithms have basically advantages in terms of complexity. First I would like to clear that both Logistic regression as well as SVM can form non linear decision surfaces and can be coupled with the kernel trick. If Logistic regression can be coupled with kernel then why use SVM?
Why does XGBoost perform better than SVM?,First reason is that XGBoos is an ensemble method that uses many trees to make a decision so it gains power by repeating itself.
What is the difference between SVM Rank and SVR (Support Vector Regression)?,One is used for ranking and the other is used for regression.
What is the difference between the normal soft margin SVM and SVM with a linear kernel?,Hard-margin
How is linear classifier relevant to SVM?,"An svm is a type of linear classifier. If you don’t mess with kernels, it’s arguably the most simple type of linear classifier."
What are the advantages of using a naive Bayes for classification?,Binomial Naive Bayes: It assumes that all our features are binary such that they take only two values. Means 0s can represent “word does not occur in the document” and 1s as “word occurs in the document”.
Are Gaussian Naive Bayes the same as binomial Naive Bayes?,Binomial Naive Bayes: It assumes that all our features are binary such that they take only two values. Means 0s can represent “word does not occur in the document” and 1s as “word occurs in the document”.
What is the difference between the Naive Bayes Classifier and the Bayes classifier?,"Naive Bayes assumes conditional independence, P(X|Y, Z)=P(X|Z)"
In what real world applications is Naive Bayes classifier used?,Some of real world examples are as given below
Is naive Bayes supervised or unsupervised?,"First, Naive Bayes is not one algorithm but a family of Algorithms that inherits the following attributes:"
What do you understand by selection bias in Machine Learning?,"Selection bias stands for the bias which was introduced by the selection of individuals, groups or data for doing analysis in a way that the proper randomization is not achieved. It ensures that the sample obtained is not representative of the population intended to be analyzed and sometimes it is referred to as the selection effect. This is the part of distortion of a statistical analysis which results from the method of collecting samples. If you don’t take the selection bias into the account then some conclusions of the study may not be accurate."
What do you understand by Precision and Recall?,"In pattern recognition, The information retrieval and classification in machine learning are part of precision. It is also called as positive predictive value which is the fraction of relevant instances among the retrieved instances."
What Are the Three Stages of Building a Model in Machine Learning?,"To build a model in machine learning, you need to follow few steps:"
How Do You Design an Email Spam Filter in Machine Learning?,"The information gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding the attribute that returns the highest information gain (i.e., the most homogeneous branches). Step 1: Calculate entropy of the target."
What is the difference between Entropy and Information Gain?,"The information gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding the attribute that returns the highest information gain (i.e., the most homogeneous branches). Step 1: Calculate entropy of the target."
What are collinearity and multicollinearity?,Collinearity is a linear association between two predictors. Multicollinearity is a situation where two or more predictors are highly linearly related.
What is Kernel SVM?,SVM algorithms have basically advantages in terms of complexity. First I would like to clear that both Logistic regression as well as SVM can form non linear decision surfaces and can be coupled with the kernel trick. If Logistic regression can be coupled with kernel then why use SVM?
What is the process of carrying out a linear regression?,Linear Regression Analysis consists of more than just fitting a linear line through a cloud of data points. It consists of 3 stages–
How do I start a career in machine learning?,"There is no fixed or definitive guide through which you can start your machine learning career. The first step is to understand the basic principles of the subject and learn a few key concepts such as algorithms and data structures, coding capabilities, calculus, linear algebra, statistics. For better data analysis, You should have clear understanding of statistics for Machine Learning. The next step would be to take up an ML course or read the top books for self-learning. You can also work on projects to get a hands-on experience."
What is the best way to learn machine learning?,"Any way that suits your style of learning can be considered as the best way to learn. Different people may enjoy different methods. Some of the common ways would be through taking up basics of machine learning course for free, watching YouTube videos, reading blogs with relevant topics, read books which can help you self-learn."
What degree do you need for machine learning?,Most hiring companies will look for a masters or doctoral degree in the relevant domain. The field of study includes computer science or mathematics. But having the necessary skills even without the degree can help you land a ML job too.
How do you break into machine learning?,"The most common way to get into a machine learning career is to acquire the necessary skills. Learn programming languages such as C, C++, Python, and Java. Gain basic knowledge about various ML algorithms, mathematical knowledge about calculus and statistics. This will help you go a long way."
How difficult is machine learning?,"Machine Learning is a vast concept that contains a lot different aspects. With the right guidance and with consistent hard-work, it may not be very difficult to learn. It definitely requires a lot of time and effort, but if you’re interested in the subject and are willing to learn, it won’t be too difficult."
What is machine learning for beginners?,"Machine Learning for beginners will consist of the basic concepts such as types of Machine Learning (Supervised, Unsupervised, Reinforcement Learning). Each of these types of ML have different algorithms and libraries within them, such as, Classification and Regression. There are various classification algorithms and regression algorithms such as Linear Regression. This would be the first thing you will learn before moving ahead with other concepts."
What level of math is required for machine learning?,"You will need to know statistical concepts, linear algebra, probability, Multivariate Calculus, Optimization. As you go into the more in-depth concepts of ML, you will need more knowledge regarding these topics."
Does machine learning require coding?,Programming is a part of Machine Learning. It is important to know programming languages such as Python.
