{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from typing import List, Tuple, Dict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import arabic_reshaper\n",
        "from bidi.algorithm import get_display\n",
        "from camel_tools.utils.normalize import normalize_unicode\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "dataset = [\n",
        "    {\n",
        "    \"question\": \"ما هي الأنواع المختلفة لنماذج التدريب في التعلم الآلي؟\",\n",
        "    \"answer\": \"في مجال التعلم الآلي، توجد ثلاثة أنواع رئيسية لنماذج التدريب: التعلم الخاضع للإشراف (Supervised Learning)، حيث يتم استخدام بيانات مُعلّمة لتوجيه النموذج؛ التعلم غير الخاضع للإشراف (Unsupervised Learning)، والذي يعتمد على اكتشاف الأنماط والعلاقات داخل البيانات غير المُعلّمة؛ والتعلم التعزيزي (Reinforcement Learning)، حيث يتعلم النموذج من خلال التفاعل مع البيئة وتعزيز السلوك الذي يؤدي إلى تحقيق الأهداف.\",\n",
        "    \"domain_terms\": [\"Supervised Learning\", \"Unsupervised Learning\", \"Reinforcement Learning\", \"التعلم الخاضع للإشراف\", \"التعلم غير الخاضع للإشراف\", \"التعلم التعزيزي\"]\n",
        "}\n",
        ",\n",
        "    {\n",
        "    \"question\": \"ما هو الفرق الرئيسي بين التعلم تحت الإشراف والتعلم غير المنظم؟\",\n",
        "    \"answer\": \"الفرق الرئيسي بين التعلم تحت الإشراف (Supervised Learning) والتعلم غير المنظم (Unsupervised Learning) يكمن في وجود أو غياب البيانات المُعلّمة. التعلم تحت الإشراف يستخدم بيانات تحتوي على مخرجات أو إجابات صحيحة لتدريب النموذج، بينما التعلم غير المنظم يعتمد على بيانات غير معلّمة لا تحتوي على مخرجات محددة. الهدف في التعلم تحت الإشراف هو بناء نموذج للتنبؤ بالمخرجات بناءً على بيانات جديدة، في حين أن الهدف في التعلم غير المنظم هو اكتشاف الأنماط والعلاقات داخل البيانات.\",\n",
        "    \"domain_terms\": [\"Supervised Learning\", \"Unsupervised Learning\", \"التعلم تحت الإشراف\", \"التعلم غير المنظم\"]\n",
        "}\n",
        ",\n",
        "    {\n",
        "    \"question\": \"ما هو الفرق بين السببية والارتباط؟\",\n",
        "    \"answer\": \"السببية تشير إلى العلاقة التي تؤدي فيها التغييرات في أحد المتغيرات إلى تغييرات في متغير آخر، حيث تكون هناك علاقة بين السبب والنتيجة. أما الارتباط فيشير إلى العلاقة بين متغيرين أو أكثر من حيث كيفية تحركهم معًا، ولكن لا يعني بالضرورة أن أحدهم يسبب الآخر. على سبيل المثال، يمكن أن يكون هناك ارتباط بين درجات الحرارة العالية وزيادة في استهلاك المثلجات، لكن هذا لا يعني أن زيادة الحرارة هي السبب في زيادة الاستهلاك.\",\n",
        "    \"domain_terms\": [\"السببية\", \"الارتباط\", \"العلاقة السببية\", \"الارتباط بين المتغيرات\"]\n",
        "}\n",
        "]\n",
        "\n",
        "# Simulated model-generated answers\n",
        "generated_answers = [\"\"\"\n",
        "    \"\n",
        "نماذج التدريب المستقبلية (Supervised Learning) : هي نوع من النماذج التي تتطلب معالجة بيانات مُحددة أو بيانات مُعلقة لتكون قادرة على تعلم خطوات تصحيح أو تحديد الخاصية.\n",
        "نماذج التدريب الغير مستقبلية (Unsupervised Learning) : هي نوع من النماذج التي تتطلب معالجة بيانات غير مُحددة أو بيانات مُجمدة، لكي تكون قادرة على التعلم بشكل خاص في معرفة خاصية جديدة أو فرز البيانات في مجموعات غير مُحددة.\n",
        "نماذج التدريب السياسية (Semi-Supervised Learning) : هي نوع من النماذج التي تتطلب معالجة بيانات مُحددة وغير مُحددة، لكي تكون قادرة على التعلم بشكل خاص في معرفة خاصية جديدة.\n",
        "نماذج التدريب المستمر (Reinforcement Learning) : هي نوع من النماذج التي تتطلب معالجة بيانات غير مُحددة أو مُحددة، لكي تكون قادرة على التعلم بشكل خاص في إجراء الاختيارات المميزة في مستوى أعلى. \"\"\"\" \",\n",
        "\n",
        "    \"\"\"أحد الفرق الرئيسي الرئيسي بين التعلم تحت الإشراف (Supervised Learning) وغير المنظم (Unsupervised Learning) هو عدم وجود مثال معلم (Labeled Data) في الخطوة الثانية.\n",
        "\n",
        "1. التعلم المنظم (Supervised Learning): يتم تدريب النموذج على مجموعة من البيانات المُعلمة (Labeled Data), حيث يكون هناك أسئلة والتي يتم عرضها للنموذج ويتم عرض إجابة صحيحة (Correct Answer) لها. بعد ذلك، يتم استخدام النموذج المُدرب للتعليم على مجموعة من البيانات التي لم تتم عرضها للنموذج أثناء التدريب (Test Data) ويتم قياس النجاح الرأسي (Accuracy) للنموذج.\n",
        "2. التعلم الغير منظم (Unsupervised Learning): لا يوجد هناك إجابة صحيحة لأسئلة النموذج. النموذج يتعلم بالنفس من البيانات، ويقوم بفرزها إلى فئات أو تحديث خصائصها.\n",
        "\n",
        "كلما زادت عدم الإشراف على البيانات، كلما زادت الصعوبة في التعليق على مشاكل حقيقية. بالطبيعة، إذا كانت المشكلة غير مُعرفة أو لا يوجد بيانات مُعلمة لها، فهناك حاجة إلى التعلم الغير منظم. في الخطوة الثانية، يتم استخدام النموذج المُدرب للتعليق على مشاكل حقيقية غير مُعرفة.\"\"\",\n",
        "                     \"\"\"\n",
        "                     نعم، إن السببية والارتباط محدودتان للأسئلة في التعلم الآلي.\n",
        "\n",
        "* **السببية** (features) هي المقاييس التي يتم تحديدها للمشكلة ويتم استخدامها في عملية التحليل. مثلًا، إذا كنت تريد تحليل أنواع سبائك جديدة، فتحدد مقاييسها المتعددة مثل الحجم النوعي، والرصيف، والمعدن الخام.\n",
        "* **الارتباط** (relationship) هو علاقة بين السببية والإرجاع (output) مثل علاقة الحجم النوعي بالسبائك والتصنيع.\n",
        "\n",
        "في التعلم الآلي، نحدد السببية ونحللها بأشكال مختلفة مثل الإحصاء الأساسي (regression) والإقرار (classification). على الإطلاق، يؤدي العمل على السببية إلى معرفة علاقتها بالإرجاع.\n",
        "\"\"\"]\n",
        "\n",
        "class ArabicTextEvaluator:\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        \"\"\"\n",
        "        Initialize the Arabic text evaluator with necessary models and tools.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the sentence transformer model to use\n",
        "        \"\"\"\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        self.smoothing = SmoothingFunction().method4\n",
        "\n",
        "    def preprocess_arabic_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Preprocess Arabic text for better evaluation.\n",
        "\n",
        "        Args:\n",
        "            text: Input Arabic text\n",
        "\n",
        "        Returns:\n",
        "            Preprocessed text\n",
        "        \"\"\"\n",
        "        # Normalize Unicode representations\n",
        "        text = normalize_unicode(text)\n",
        "\n",
        "        # Remove diacritics and tatweel\n",
        "        text = re.sub(r'[\\u064B-\\u065F\\u0640]', '', text)\n",
        "\n",
        "        # Normalize Arabic characters\n",
        "        replacements = {\n",
        "            'أ': 'ا', 'إ': 'ا', 'آ': 'ا',\n",
        "            'ى': 'ي', 'ة': 'ه'\n",
        "        }\n",
        "        for old, new in replacements.items():\n",
        "            text = text.replace(old, new)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def compute_semantic_coherence(self, generated: str, reference: str) -> float:\n",
        "        \"\"\"\n",
        "        Compute semantic coherence between generated and reference texts.\n",
        "\n",
        "        Args:\n",
        "            generated: Generated Arabic text\n",
        "            reference: Reference Arabic text\n",
        "\n",
        "        Returns:\n",
        "            Semantic coherence score\n",
        "        \"\"\"\n",
        "        gen_emb = self.embedding_model.encode([generated], convert_to_tensor=True)\n",
        "        ref_emb = self.embedding_model.encode([reference], convert_to_tensor=True)\n",
        "        return float(cosine_similarity(gen_emb, ref_emb)[0][0])\n",
        "\n",
        "    def compute_domain_coverage(\n",
        "        self,\n",
        "        text: str,\n",
        "        domain_terms: List[str],\n",
        "        weights: Dict[str, float] = None\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Compute domain-specific terminology coverage with term importance weighting.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "            domain_terms: List of domain-specific terms\n",
        "            weights: Optional term importance weights\n",
        "\n",
        "        Returns:\n",
        "            Domain coverage score\n",
        "        \"\"\"\n",
        "        if weights is None:\n",
        "            weights = {term: 1.0 for term in domain_terms}\n",
        "\n",
        "        text_tokens = set(simple_word_tokenize(text))\n",
        "        covered_terms = text_tokens.intersection(set(domain_terms))\n",
        "\n",
        "        weighted_coverage = sum(weights.get(term, 1.0) for term in covered_terms)\n",
        "        total_weight = sum(weights.values())\n",
        "\n",
        "        return weighted_coverage / total_weight if total_weight > 0 else 0.0\n",
        "\n",
        "    def compute_enhanced_wssa(\n",
        "        self,\n",
        "        generated_answers: List[str],\n",
        "        correct_answers: List[str],\n",
        "        domain_terms: List[List[str]],\n",
        "        weights: Dict[str, float] = None\n",
        "    ) -> List[Dict[str, float]]:\n",
        "        \"\"\"\n",
        "        Compute Enhanced WSSA (Weighted Semantic Similarity with Arabic-specific Adjustments)\n",
        "\n",
        "        Args:\n",
        "            generated_answers: List of generated answers\n",
        "            correct_answers: List of reference answers\n",
        "            domain_terms: List of domain-specific terms for each answer\n",
        "            weights: Optional component weights\n",
        "\n",
        "        Returns:\n",
        "            List of evaluation metrics for each answer\n",
        "        \"\"\"\n",
        "        if weights is None:\n",
        "            weights = {\n",
        "                'semantic': 0.35,\n",
        "                'domain': 0.25,\n",
        "                'fluency': 0.15,\n",
        "                'bleu': 0.15,\n",
        "                'rouge': 0.10\n",
        "            }\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for gen_ans, corr_ans, terms in zip(generated_answers, correct_answers, domain_terms):\n",
        "            # Preprocess texts\n",
        "            gen_ans_proc = self.preprocess_arabic_text(gen_ans)\n",
        "            corr_ans_proc = self.preprocess_arabic_text(corr_ans)\n",
        "\n",
        "            # Component scores\n",
        "            semantic_score = self.compute_semantic_coherence(gen_ans_proc, corr_ans_proc)\n",
        "            domain_score = self.compute_domain_coverage(gen_ans_proc, terms)\n",
        "\n",
        "            # BLEU score with Arabic-specific tokenization\n",
        "            bleu_score = sentence_bleu(\n",
        "                [simple_word_tokenize(corr_ans_proc)],\n",
        "                simple_word_tokenize(gen_ans_proc),\n",
        "                smoothing_function=self.smoothing\n",
        "            )\n",
        "\n",
        "            # ROUGE scores\n",
        "            rouge_scores = self.rouge_scorer.score(corr_ans_proc, gen_ans_proc)\n",
        "            rouge_l = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "            # Compute weighted final score\n",
        "            final_score = (\n",
        "                weights['semantic'] * semantic_score +\n",
        "                weights['domain'] * domain_score +\n",
        "                weights['bleu'] * bleu_score +\n",
        "                weights['rouge'] * rouge_l\n",
        "            )\n",
        "\n",
        "            metrics = {\n",
        "                'final_score': final_score,\n",
        "                'semantic_coherence': semantic_score,\n",
        "                'domain_coverage': domain_score,\n",
        "                'bleu': bleu_score,\n",
        "                'rouge_l': rouge_l\n",
        "            }\n",
        "\n",
        "            results.append(metrics)\n",
        "\n",
        "        return results\n",
        "correct_answers = [item[\"answer\"] for item in dataset]\n",
        "domain_terms = [item[\"domain_terms\"] for item in dataset]\n",
        "# Example usage\n",
        "evaluator = ArabicTextEvaluator()\n",
        "results = evaluator.compute_enhanced_wssa(generated_answers, correct_answers, domain_terms)\n",
        "\n",
        "# Display results\n",
        "for i, metrics in enumerate(results, 1):\n",
        "    print(f\"\\nQuestion {i} Evaluation Metrics:\")\n",
        "    for metric, score in metrics.items():\n",
        "        print(f\"{metric}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "QMzOSRB3mX8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a688b837-ad45-48aa-cc90-85cc805edc0e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 1 Evaluation Metrics:\n",
            "final_score: 0.3978\n",
            "semantic_coherence: 0.8830\n",
            "domain_coverage: 0.0000\n",
            "bleu: 0.0580\n",
            "rouge_l: 0.8000\n",
            "\n",
            "Question 2 Evaluation Metrics:\n",
            "final_score: 0.3472\n",
            "semantic_coherence: 0.8573\n",
            "domain_coverage: 0.0000\n",
            "bleu: 0.0826\n",
            "rouge_l: 0.3478\n",
            "\n",
            "Question 3 Evaluation Metrics:\n",
            "final_score: 0.3888\n",
            "semantic_coherence: 0.9265\n",
            "domain_coverage: 0.2500\n",
            "bleu: 0.0132\n",
            "rouge_l: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QpUVO7YveGnJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}