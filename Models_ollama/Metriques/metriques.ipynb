{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJnQpyEbVtVw"
      },
      "outputs": [],
      "source": [
        "\"\"\"!pip install arabic_reshaper\n",
        "!pip install python-bidi\n",
        "!pip install camel-tools\n",
        "!pip install rouge-score\n",
        "!pip install nltk\"\"\"\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from typing import List, Tuple, Dict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import arabic_reshaper\n",
        "from bidi.algorithm import get_display\n",
        "from camel_tools.utils.normalize import normalize_unicode\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "generated_answers_llama = ['هناك العديد من النماذج المختلفة لنماذج التدريب في التعلم الآلي، وتختلف حسب التطبيق والموضوع. بعض النماذج الشائعة تشمل:\\n\\n1. **النماذج الخطية**: وهي أكثر نماذج التدريب استخدامًا وسهلة التحليل. تتميز بتحديد العوامل المتعلقة بالدالة من خلال معادلة خطية.\\n\\n2. **النماذج اللينية**: تشبه النماذج الخطية، ولكنها توفر مرونة أكثر وتستطيع التعامل مع البيانات المتراكمة أو غير المتسلسلة.\\n\\n3. **النماذج العشوائية**: تستخدم الارتباطات العشوائية لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\n4. **النماذج الدورانية**: تستخدم توجيهات دورية لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\n5. **النماذج الجريئة**: تستخدم تقنيات جريئة لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\n6. **النماذج الربطية**: تستخدم تقنيات ربط لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة bigger في التعامل مع البيانات غير المتراكمة.\\n\\n7. **النماذج الشبكية**: تستخدم شبكات عصبية لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\n8. **النماذج الدورية**: تستخدم تقنيات دورية لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\n9. **النماذج الجرية**: تستخدم تقنيات جريئة لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\n10. **النماذج الربطية**: تستخدم تقنيات ربط لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\nهذه قائمة ببعض الأنواع المختلفة لنماذج التدريب في التعلم الآلي. تختلف كل نوع من النماذج حسب التطبيق والموضوع.', 'الفرق الرئيسية بين التعلم تحت الإشراف والتعلم غير المنظم هي:\\n\\n1- تعلم تحت الإشراف: يعتمد على وجود قائد أو مدرب منعزل، ويستند على التفاعل بشكل مباشر مع المرضى. في هذا النوع من التعلم، يتعلم المرضى من خلال الدروس والأنشطة التي تُقدمها القيد أو المدرب، ويمتصفون المعلومات والمهارات من خلال التفاعل المستمر معهم.\\n\\n2- تعلم غير المنظم: يعتمد على وجود قائد أو مدرب منعزلاً، ولكن لا يستند إلى التفاعل بشكل مباشر مع المرضى. في هذا النوع من التعلم، تُقدم الدروس والأنشطة التي تُقدمها القيد أو المدرب، ويمتصف المرضى المعلومات والمهارات من خلال الاستماع والقراءة أو استخدام الموارد المتاحة بلا وجود للتفاعل المستمر معهم.', 'السببية وتشابهها بالارتباط هي مصطلحين يُستخدمان في الفلسفة والفيزياء لوصف العلاقات بين الأحداث أو events. بينهما فرق مهم.\\n\\nالسببية:\\nتُعد السببية مصدرًا للعلاقة بين الأحداث، حيث أن حدث A يكون سببًا لحدث B إذا كانت الحدث B تؤدي إلى حدث A. في هذه الحالة، يعتبر حدث A السببي لحدث B، ويعني أن حدث B لا يمكن أن يحدث دون حدث A.\\n\\nالارتباط:\\nيُعد الارتباط مصدرًا للعلاقة بين الأحداث، ولكن لا يعني أن أحد الأحداث هو سبب للآخر. في هذه الحالة، يعتبر وجود علاقة بين الأحداث من غير المرجح أو غير متوقع، ويمكن أن يكون هناك عدة عوامل تؤثر على هذه العلاقة.\\n\\nمثال:\\n\\n*   السببية: إذا كان حدث A يسبب حدث B، فمن المحتمل أن حدث B سيكون سببًا لحدث C.\\n*   الارتباط: إذا كان هناك علاقة بين الأحداث A و B، ولكن لا توجد سببية مباشرة بينهما، فإن العلاقة بينهم يمكن أن تكون من غير المرجح أو غير متوقع.\\n\\nفي النهاية، السببية تعني أن حدث A هو سبب لحدث B، بينما الارتباط يعني أن هناك علاقة بين الأحداث ولكن لا توجد سببية مباشرة.']\n",
        "\n",
        "dataset = [\n",
        "    {\n",
        "        \"question\": \"ما هي الأنواع المختلفة لنماذج التدريب في التعلم الآلي؟\",\n",
        "        \"answer\": (\n",
        "            \"في مجال التعلم الآلي، توجد ثلاثة أنواع رئيسية لنماذج التدريب:\\n\"\n",
        "            \"1. **التعلم الخاضع للإشراف** (Supervised Learning): حيث يتم استخدام بيانات مُعلّمة لتوجيه النموذج. هذا النوع من التعلم يعتمد على وجود بيانات مُدخلة ومخرجات معروفة، ويهدف إلى تعلم النموذج على التنبؤ بالمخرجات بناءً على المدخلات الجديدة.\\n\"\n",
        "            \"2. **التعلم غير الخاضع للإشراف** (Unsupervised Learning): والذي يعتمد على اكتشاف الأنماط والعلاقات داخل البيانات غير المُعلّمة. هذا النوع من التعلم يهدف إلى اكتشاف البنية الكامنة في البيانات، مثل المجموعات أو العناقيد، دون وجود مخرجات معروفة.\\n\"\n",
        "            \"3. **التعلم التعزيزي** (Reinforcement Learning): حيث يتعلم النموذج من خلال التفاعل مع البيئة وتعزيز السلوك الذي يؤدي إلى تحقيق الأهداف. هذا النوع من التعلم يعتمد على وجود وكيل يتفاعل مع البيئة، ويهدف إلى تعلم النموذج على اتخاذ القرارات التي تؤدي إلى تحقيق الأهداف.\"\n",
        "        ),\n",
        "        \"domain_terms\": [\n",
        "            \"Supervised Learning\",\n",
        "            \"Unsupervised Learning\",\n",
        "            \"Reinforcement Learning\",\n",
        "            \"التعلم الخاضع للإشراف\",\n",
        "            \"التعلم غير الخاضع للإشراف\",\n",
        "            \"التعلم التعزيزي\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"ما هو الفرق الرئيسي بين التعلم تحت الإشراف والتعلم غير المنظم؟\",\n",
        "        \"answer\": (\n",
        "            \"الفرق الرئيسي بين التعلم تحت الإشراف (Supervised Learning) والتعلم غير المنظم (Unsupervised Learning) يكمن في وجود أو غياب البيانات المُعلّمة.\\n\"\n",
        "            \"* التعلم تحت الإشراف يستخدم بيانات تحتوي على مخرجات أو إجابات صحيحة لتدريب النموذج. الهدف في هذا النوع من التعلم هو بناء نموذج للتنبؤ بالمخرجات بناءً على بيانات جديدة.\\n\"\n",
        "            \"* التعلم غير المنظم يعتمد على بيانات غير معلّمة لا تحتوي على مخرجات محددة. الهدف في هذا النوع من التعلم هو اكتشاف الأنماط والعلاقات داخل البيانات.\"\n",
        "        ),\n",
        "        \"domain_terms\": [\n",
        "            \"Supervised Learning\",\n",
        "            \"Unsupervised Learning\",\n",
        "            \"التعلم تحت الإشراف\",\n",
        "            \"التعلم غير المنظم\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"ما هو الفرق بين السببية والارتباط؟\",\n",
        "        \"answer\": (\n",
        "            \"السببية تشير إلى العلاقة التي تؤدي فيها التغييرات في أحد المتغيرات إلى تغييرات في متغير آخر، حيث تكون هناك علاقة بين السبب والنتيجة.\\n\"\n",
        "            \"* على سبيل المثال، يمكن أن يكون هناك علاقة سببية بين التدخين والإصابة بسرطان الرئة، حيث يؤدي التدخين إلى زيادة خطر الإصابة بسرطان الرئة.\\n\"\n",
        "            \"أما الارتباط فيشير إلى العلاقة بين متغيرين أو أكثر من حيث كيفية تحركهم معًا، ولكن لا يعني بالضرورة أن أحدهم يسبب الآخر.\\n\"\n",
        "            \"* على سبيل المثال، يمكن أن يكون هناك ارتباط بين درجات الحرارة العالية وزيادة في استهلاك المثلجات، لكن هذا لا يعني أن زيادة الحرارة هي السبب في زيادة الاستهلاك.\"\n",
        "        ),\n",
        "        \"domain_terms\": [\n",
        "            \"السببية\",\n",
        "            \"الارتباط\",\n",
        "            \"العلاقة السببية\",\n",
        "            \"الارتباط بين المتغيرات\"\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "generated_answers_mistral7b = [\"\"\"\n",
        "    \"\n",
        "نماذج التدريب المستقبلية (Supervised Learning) : هي نوع من النماذج التي تتطلب معالجة بيانات مُحددة أو بيانات مُعلقة لتكون قادرة على تعلم خطوات تصحيح أو تحديد الخاصية.\n",
        "نماذج التدريب الغير مستقبلية (Unsupervised Learning) : هي نوع من النماذج التي تتطلب معالجة بيانات غير مُحددة أو بيانات مُجمدة، لكي تكون قادرة على التعلم بشكل خاص في معرفة خاصية جديدة أو فرز البيانات في مجموعات غير مُحددة.\n",
        "نماذج التدريب السياسية (Semi-Supervised Learning) : هي نوع من النماذج التي تتطلب معالجة بيانات مُحددة وغير مُحددة، لكي تكون قادرة على التعلم بشكل خاص في معرفة خاصية جديدة.\n",
        "نماذج التدريب المستمر (Reinforcement Learning) : هي نوع من النماذج التي تتطلب معالجة بيانات غير مُحددة أو مُحددة، لكي تكون قادرة على التعلم بشكل خاص في إجراء الاختيارات المميزة في مستوى أعلى. \"\"\"\" \",\n",
        "\n",
        "    \"\"\"أحد الفرق الرئيسي الرئيسي بين التعلم تحت الإشراف (Supervised Learning) وغير المنظم (Unsupervised Learning) هو عدم وجود مثال معلم (Labeled Data) في الخطوة الثانية.\n",
        "\n",
        "1. التعلم المنظم (Supervised Learning): يتم تدريب النموذج على مجموعة من البيانات المُعلمة (Labeled Data), حيث يكون هناك أسئلة والتي يتم عرضها للنموذج ويتم عرض إجابة صحيحة (Correct Answer) لها. بعد ذلك، يتم استخدام النموذج المُدرب للتعليم على مجموعة من البيانات التي لم تتم عرضها للنموذج أثناء التدريب (Test Data) ويتم قياس النجاح الرأسي (Accuracy) للنموذج.\n",
        "2. التعلم الغير منظم (Unsupervised Learning): لا يوجد هناك إجابة صحيحة لأسئلة النموذج. النموذج يتعلم بالنفس من البيانات، ويقوم بفرزها إلى فئات أو تحديث خصائصها.\n",
        "\n",
        "كلما زادت عدم الإشراف على البيانات، كلما زادت الصعوبة في التعليق على مشاكل حقيقية. بالطبيعة، إذا كانت المشكلة غير مُعرفة أو لا يوجد بيانات مُعلمة لها، فهناك حاجة إلى التعلم الغير منظم. في الخطوة الثانية، يتم استخدام النموذج المُدرب للتعليق على مشاكل حقيقية غير مُعرفة.\"\"\",\n",
        "                     \"\"\"\n",
        "                     نعم، إن السببية والارتباط محدودتان للأسئلة في التعلم الآلي.\n",
        "\n",
        "* **السببية** (features) هي المقاييس التي يتم تحديدها للمشكلة ويتم استخدامها في عملية التحليل. مثلًا، إذا كنت تريد تحليل أنواع سبائك جديدة، فتحدد مقاييسها المتعددة مثل الحجم النوعي، والرصيف، والمعدن الخام.\n",
        "* **الارتباط** (relationship) هو علاقة بين السببية والإرجاع (output) مثل علاقة الحجم النوعي بالسبائك والتصنيع.\n",
        "\n",
        "في التعلم الآلي، نحدد السببية ونحللها بأشكال مختلفة مثل الإحصاء الأساسي (regression) والإقرار (classification). على الإطلاق، يؤدي العمل على السببية إلى معرفة علاقتها بالإرجاع.\n",
        "\"\"\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ArabicTextEvaluator:\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        \"\"\"\n",
        "        Initialize the Arabic text evaluator with necessary models and tools.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the sentence transformer model to use\n",
        "        \"\"\"\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        self.smoothing = SmoothingFunction().method4\n",
        "\n",
        "    def preprocess_arabic_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Preprocess Arabic text for better evaluation.\n",
        "\n",
        "        Args:\n",
        "            text: Input Arabic text\n",
        "\n",
        "        Returns:\n",
        "            Preprocessed text\n",
        "        \"\"\"\n",
        "        # Normalize Unicode representations\n",
        "        text = normalize_unicode(text)\n",
        "\n",
        "        # Remove diacritics and tatweel\n",
        "        text = re.sub(r'[\\u064B-\\u065F\\u0640]', '', text)\n",
        "\n",
        "        # Normalize Arabic characters\n",
        "        replacements = {\n",
        "            'أ': 'ا', 'إ': 'ا', 'آ': 'ا',\n",
        "            'ى': 'ي', 'ة': 'ه'\n",
        "        }\n",
        "        for old, new in replacements.items():\n",
        "            text = text.replace(old, new)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def compute_semantic_coherence(self, generated: str, reference: str) -> float:\n",
        "        \"\"\"\n",
        "        Compute semantic coherence between generated and reference texts.\n",
        "\n",
        "        Args:\n",
        "            generated: Generated Arabic text\n",
        "            reference: Reference Arabic text\n",
        "\n",
        "        Returns:\n",
        "            Semantic coherence score\n",
        "        \"\"\"\n",
        "        gen_emb = self.embedding_model.encode([generated], convert_to_tensor=True)\n",
        "        ref_emb = self.embedding_model.encode([reference], convert_to_tensor=True)\n",
        "        return float(cosine_similarity(gen_emb, ref_emb)[0][0])\n",
        "\n",
        "    def compute_domain_coverage(\n",
        "        self,\n",
        "        text: str,\n",
        "        domain_terms: List[str],\n",
        "        weights: Dict[str, float] = None\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Compute domain-specific terminology coverage with term importance weighting.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "            domain_terms: List of domain-specific terms\n",
        "            weights: Optional term importance weights\n",
        "\n",
        "        Returns:\n",
        "            Domain coverage score\n",
        "        \"\"\"\n",
        "        if weights is None:\n",
        "            weights = {term: 1.0 for term in domain_terms}\n",
        "\n",
        "        text_tokens = set(simple_word_tokenize(text))\n",
        "        covered_terms = text_tokens.intersection(set(domain_terms))\n",
        "\n",
        "        weighted_coverage = sum(weights.get(term, 1.0) for term in covered_terms)\n",
        "        total_weight = sum(weights.values())\n",
        "\n",
        "        return weighted_coverage / total_weight if total_weight > 0 else 0.0\n",
        "\n",
        "    def compute_enhanced_wssa(\n",
        "        self,\n",
        "        generated_answers: List[str],\n",
        "        reference_answers: List[str],\n",
        "        domain_terms: List[List[str]],\n",
        "        weights: Dict[str, float] = None\n",
        "    ) -> List[Dict[str, float]]:\n",
        "        \"\"\"\n",
        "        Compute Enhanced WSSA (Weighted Semantic Similarity with Arabic-specific Adjustments)\n",
        "\n",
        "        Args:\n",
        "            generated_answers: List of generated answers\n",
        "            reference_answers: List of reference answers\n",
        "            domain_terms: List of domain-specific terms for each answer\n",
        "            weights: Optional component weights\n",
        "\n",
        "        Returns:\n",
        "            List of evaluation metrics for each answer\n",
        "        \"\"\"\n",
        "        if weights is None:\n",
        "            weights = {\n",
        "                'semantic': 0.35,\n",
        "                'domain': 0.25,\n",
        "                'fluency': 0.15,\n",
        "                'bleu': 0.15,\n",
        "                'rouge': 0.10\n",
        "            }\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for gen_ans, ref_ans, terms in zip(generated_answers, reference_answers, domain_terms):\n",
        "            # Preprocess texts\n",
        "            gen_ans_proc = self.preprocess_arabic_text(gen_ans)\n",
        "            ref_ans_proc = self.preprocess_arabic_text(ref_ans)\n",
        "\n",
        "            # Component scores\n",
        "            semantic_score = self.compute_semantic_coherence(gen_ans_proc, ref_ans_proc)\n",
        "            domain_score = self.compute_domain_coverage(gen_ans_proc, terms)\n",
        "\n",
        "            # BLEU score with Arabic-specific tokenization\n",
        "            bleu_score = sentence_bleu(\n",
        "                [simple_word_tokenize(ref_ans_proc)],\n",
        "                simple_word_tokenize(gen_ans_proc),\n",
        "                smoothing_function=self.smoothing\n",
        "            )\n",
        "\n",
        "            # ROUGE scores\n",
        "            rouge_scores = self.rouge_scorer.score(ref_ans_proc, gen_ans_proc)\n",
        "            rouge_l = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "            # Compute weighted final score\n",
        "            final_score = (\n",
        "                weights['semantic'] * semantic_score +\n",
        "                weights['domain'] * domain_score +\n",
        "                weights['bleu'] * bleu_score +\n",
        "                weights['rouge'] * rouge_l\n",
        "            )\n",
        "\n",
        "            metrics = {\n",
        "                'final_score': final_score,\n",
        "                'semantic_coherence': semantic_score,\n",
        "                'domain_coverage': domain_score,\n",
        "                'bleu': bleu_score,\n",
        "                'rouge_l': rouge_l\n",
        "            }\n",
        "\n",
        "            results.append(metrics)\n",
        "\n",
        "        return results\n",
        "\n",
        "# Example usage\n",
        "evaluator = ArabicTextEvaluator()\n",
        "\n",
        "# Compute results for both models\n",
        "results_llama = evaluator.compute_enhanced_wssa(generated_answers_llama, correct_answers, domain_terms)\n",
        "results_mistral7b = evaluator.compute_enhanced_wssa(generated_answers_mistral7b, correct_answers, domain_terms)\n",
        "\n",
        "# Display results for each model\n",
        "print(\"\\nResults for LLaMA:\")\n",
        "for i, metrics in enumerate(results_llama, 1):\n",
        "    print(f\"\\nQuestion {i} Evaluation Metrics:\")\n",
        "    for metric, score in metrics.items():\n",
        "        print(f\"{metric}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axPKs7tZ5NxF",
        "outputId": "9b0f7fcc-a70f-41b9-a0a8-f91f700d34ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for LLaMA:\n",
            "\n",
            "Question 1 Evaluation Metrics:\n",
            "final_score: 0.3163\n",
            "semantic_coherence: 0.8001\n",
            "domain_coverage: 0.0000\n",
            "bleu: 0.0413\n",
            "rouge_l: 0.3000\n",
            "\n",
            "Question 2 Evaluation Metrics:\n",
            "final_score: 0.3225\n",
            "semantic_coherence: 0.8749\n",
            "domain_coverage: 0.0000\n",
            "bleu: 0.1083\n",
            "rouge_l: 0.0000\n",
            "\n",
            "Question 3 Evaluation Metrics:\n",
            "final_score: 0.3425\n",
            "semantic_coherence: 0.7832\n",
            "domain_coverage: 0.2500\n",
            "bleu: 0.0390\n",
            "rouge_l: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nResults for Mistral7B:\")\n",
        "for i, metrics in enumerate(results_mistral7b, 1):\n",
        "    print(f\"\\nQuestion {i} Evaluation Metrics:\")\n",
        "    for metric, score in metrics.items():\n",
        "        print(f\"{metric}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vugp4RNr63Xc",
        "outputId": "e57a0927-b7e8-40a8-afb4-90bfda387e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for Mistral7B:\n",
            "\n",
            "Question 1 Evaluation Metrics:\n",
            "final_score: 0.3662\n",
            "semantic_coherence: 0.8205\n",
            "domain_coverage: 0.0000\n",
            "bleu: 0.0820\n",
            "rouge_l: 0.6667\n",
            "\n",
            "Question 2 Evaluation Metrics:\n",
            "final_score: 0.3461\n",
            "semantic_coherence: 0.8538\n",
            "domain_coverage: 0.0000\n",
            "bleu: 0.0835\n",
            "rouge_l: 0.3478\n",
            "\n",
            "Question 3 Evaluation Metrics:\n",
            "final_score: 0.3900\n",
            "semantic_coherence: 0.9289\n",
            "domain_coverage: 0.2500\n",
            "bleu: 0.0159\n",
            "rouge_l: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compare results between models\n",
        "print(\"\\nComparison between LLaMA and Mistral7B:\")\n",
        "comparison_results = []\n",
        "for i, (llama_metrics, mistral_metrics) in enumerate(zip(results_llama, results_mistral7b), 1):\n",
        "    comparison = {\n",
        "        'question': i,\n",
        "        'better_model': 'LLaMA' if llama_metrics['final_score'] > mistral_metrics['final_score'] else 'Mistral7B',\n",
        "        'llama_final_score': llama_metrics['final_score'],\n",
        "        'mistral_final_score': mistral_metrics['final_score'],\n",
        "        'semantic_difference': llama_metrics['semantic_coherence'] - mistral_metrics['semantic_coherence'],\n",
        "        'domain_difference': llama_metrics['domain_coverage'] - mistral_metrics['domain_coverage'],\n",
        "        'bleu_difference': llama_metrics['bleu'] - mistral_metrics['bleu'],\n",
        "        'rouge_difference': llama_metrics['rouge_l'] - mistral_metrics['rouge_l']\n",
        "    }\n",
        "    comparison_results.append(comparison)\n",
        "\n",
        "    print(f\"\\nQuestion {i} Comparison:\")\n",
        "    print(f\"Better Model: {comparison['better_model']}\")\n",
        "    print(f\"LLaMA Final Score: {comparison['llama_final_score']:.4f}\")\n",
        "    print(f\"Mistral7B Final Score: {comparison['mistral_final_score']:.4f}\")\n",
        "    print(f\"Semantic Coherence Difference: {comparison['semantic_difference']:.4f}\")\n",
        "    print(f\"Domain Coverage Difference: {comparison['domain_difference']:.4f}\")\n",
        "    print(f\"BLEU Difference: {comparison['bleu_difference']:.4f}\")\n",
        "    print(f\"ROUGE-L Difference: {comparison['rouge_difference']:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rsQhZtQr7ggp",
        "outputId": "4354dbc7-ddcb-42b1-c860-d79ea719d9c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison between LLaMA and Mistral7B:\n",
            "\n",
            "Question 1 Comparison:\n",
            "Better Model: Mistral7B\n",
            "LLaMA Final Score: 0.3163\n",
            "Mistral7B Final Score: 0.3662\n",
            "Semantic Coherence Difference: -0.0204\n",
            "Domain Coverage Difference: 0.0000\n",
            "BLEU Difference: -0.0407\n",
            "ROUGE-L Difference: -0.3667\n",
            "\n",
            "Question 2 Comparison:\n",
            "Better Model: Mistral7B\n",
            "LLaMA Final Score: 0.3225\n",
            "Mistral7B Final Score: 0.3461\n",
            "Semantic Coherence Difference: 0.0211\n",
            "Domain Coverage Difference: 0.0000\n",
            "BLEU Difference: 0.0248\n",
            "ROUGE-L Difference: -0.3478\n",
            "\n",
            "Question 3 Comparison:\n",
            "Better Model: Mistral7B\n",
            "LLaMA Final Score: 0.3425\n",
            "Mistral7B Final Score: 0.3900\n",
            "Semantic Coherence Difference: -0.1457\n",
            "Domain Coverage Difference: 0.0000\n",
            "BLEU Difference: 0.0230\n",
            "ROUGE-L Difference: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GLwT1TqY7imu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}