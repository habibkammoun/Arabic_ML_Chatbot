{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OJnQpyEbVtVw"
      },
      "outputs": [],
      "source": [
        "\"\"\"!pip install arabic_reshaper\n",
        "!pip install python-bidi\n",
        "!pip install camel-tools\n",
        "!pip install rouge-score\n",
        "!pip install nltk\"\"\"\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from typing import List, Tuple, Dict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import arabic_reshaper\n",
        "from bidi.algorithm import get_display\n",
        "from camel_tools.utils.normalize import normalize_unicode\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "generated_answers_llama = ['هناك العديد من النماذج المختلفة لنماذج التدريب في التعلم الآلي، وتختلف حسب التطبيق والموضوع. بعض النماذج الشائعة تشمل:\\n\\n1. **النماذج الخطية**: وهي أكثر نماذج التدريب استخدامًا وسهلة التحليل. تتميز بتحديد العوامل المتعلقة بالدالة من خلال معادلة خطية.\\n\\n2. **النماذج اللينية**: تشبه النماذج الخطية، ولكنها توفر مرونة أكثر وتستطيع التعامل مع البيانات المتراكمة أو غير المتسلسلة.\\n\\n3. **النماذج العشوائية**: تستخدم الارتباطات العشوائية لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\n4. **النماذج الدورانية**: تستخدم توجيهات دورية لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\n5. **النماذج الجريئة**: تستخدم تقنيات جريئة لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\n6. **النماذج الربطية**: تستخدم تقنيات ربط لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة bigger في التعامل مع البيانات غير المتراكمة.\\n\\n7. **النماذج الشبكية**: تستخدم شبكات عصبية لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\n8. **النماذج الدورية**: تستخدم تقنيات دورية لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\n9. **النماذج الجرية**: تستخدم تقنيات جريئة لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\n10. **النماذج الربطية**: تستخدم تقنيات ربط لتحديد العوامل المتعلقة بالدالة. يمكن أن تكون هذه النماذج أكثر تعقيدًا، ولكنها توفر مرونة أكبر في التعامل مع البيانات غير المتراكمة.\\n\\nهذه قائمة ببعض الأنواع المختلفة لنماذج التدريب في التعلم الآلي. تختلف كل نوع من النماذج حسب التطبيق والموضوع.', 'الفرق الرئيسية بين التعلم تحت الإشراف والتعلم غير المنظم هي:\\n\\n1- تعلم تحت الإشراف: يعتمد على وجود قائد أو مدرب منعزل، ويستند على التفاعل بشكل مباشر مع المرضى. في هذا النوع من التعلم، يتعلم المرضى من خلال الدروس والأنشطة التي تُقدمها القيد أو المدرب، ويمتصفون المعلومات والمهارات من خلال التفاعل المستمر معهم.\\n\\n2- تعلم غير المنظم: يعتمد على وجود قائد أو مدرب منعزلاً، ولكن لا يستند إلى التفاعل بشكل مباشر مع المرضى. في هذا النوع من التعلم، تُقدم الدروس والأنشطة التي تُقدمها القيد أو المدرب، ويمتصف المرضى المعلومات والمهارات من خلال الاستماع والقراءة أو استخدام الموارد المتاحة بلا وجود للتفاعل المستمر معهم.', 'السببية وتشابهها بالارتباط هي مصطلحين يُستخدمان في الفلسفة والفيزياء لوصف العلاقات بين الأحداث أو events. بينهما فرق مهم.\\n\\nالسببية:\\nتُعد السببية مصدرًا للعلاقة بين الأحداث، حيث أن حدث A يكون سببًا لحدث B إذا كانت الحدث B تؤدي إلى حدث A. في هذه الحالة، يعتبر حدث A السببي لحدث B، ويعني أن حدث B لا يمكن أن يحدث دون حدث A.\\n\\nالارتباط:\\nيُعد الارتباط مصدرًا للعلاقة بين الأحداث، ولكن لا يعني أن أحد الأحداث هو سبب للآخر. في هذه الحالة، يعتبر وجود علاقة بين الأحداث من غير المرجح أو غير متوقع، ويمكن أن يكون هناك عدة عوامل تؤثر على هذه العلاقة.\\n\\nمثال:\\n\\n*   السببية: إذا كان حدث A يسبب حدث B، فمن المحتمل أن حدث B سيكون سببًا لحدث C.\\n*   الارتباط: إذا كان هناك علاقة بين الأحداث A و B، ولكن لا توجد سببية مباشرة بينهما، فإن العلاقة بينهم يمكن أن تكون من غير المرجح أو غير متوقع.\\n\\nفي النهاية، السببية تعني أن حدث A هو سبب لحدث B، بينما الارتباط يعني أن هناك علاقة بين الأحداث ولكن لا توجد سببية مباشرة.']\n",
        "\n",
        "dataset = [\n",
        "    {\n",
        "        \"question\": \"ما هي الأنواع المختلفة لنماذج التدريب في التعلم الآلي؟\",\n",
        "        \"answer\": (\n",
        "            \"في مجال التعلم الآلي، توجد ثلاثة أنواع رئيسية لنماذج التدريب:\\n\"\n",
        "            \"1. **التعلم الخاضع للإشراف** (Supervised Learning): حيث يتم استخدام بيانات مُعلّمة لتوجيه النموذج. هذا النوع من التعلم يعتمد على وجود بيانات مُدخلة ومخرجات معروفة، ويهدف إلى تعلم النموذج على التنبؤ بالمخرجات بناءً على المدخلات الجديدة.\\n\"\n",
        "            \"2. **التعلم غير الخاضع للإشراف** (Unsupervised Learning): والذي يعتمد على اكتشاف الأنماط والعلاقات داخل البيانات غير المُعلّمة. هذا النوع من التعلم يهدف إلى اكتشاف البنية الكامنة في البيانات، مثل المجموعات أو العناقيد، دون وجود مخرجات معروفة.\\n\"\n",
        "            \"3. **التعلم التعزيزي** (Reinforcement Learning): حيث يتعلم النموذج من خلال التفاعل مع البيئة وتعزيز السلوك الذي يؤدي إلى تحقيق الأهداف. هذا النوع من التعلم يعتمد على وجود وكيل يتفاعل مع البيئة، ويهدف إلى تعلم النموذج على اتخاذ القرارات التي تؤدي إلى تحقيق الأهداف.\"\n",
        "        ),\n",
        "        \"domain_terms\": [\n",
        "            \"Supervised Learning\",\n",
        "            \"Unsupervised Learning\",\n",
        "            \"Reinforcement Learning\",\n",
        "            \"التعلم الخاضع للإشراف\",\n",
        "            \"التعلم غير الخاضع للإشراف\",\n",
        "            \"التعلم التعزيزي\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"ما هو الفرق الرئيسي بين التعلم تحت الإشراف والتعلم غير المنظم؟\",\n",
        "        \"answer\": (\n",
        "            \"الفرق الرئيسي بين التعلم تحت الإشراف (Supervised Learning) والتعلم غير المنظم (Unsupervised Learning) يكمن في وجود أو غياب البيانات المُعلّمة.\\n\"\n",
        "            \"* التعلم تحت الإشراف يستخدم بيانات تحتوي على مخرجات أو إجابات صحيحة لتدريب النموذج. الهدف في هذا النوع من التعلم هو بناء نموذج للتنبؤ بالمخرجات بناءً على بيانات جديدة.\\n\"\n",
        "            \"* التعلم غير المنظم يعتمد على بيانات غير معلّمة لا تحتوي على مخرجات محددة. الهدف في هذا النوع من التعلم هو اكتشاف الأنماط والعلاقات داخل البيانات.\"\n",
        "        ),\n",
        "        \"domain_terms\": [\n",
        "            \"Supervised Learning\",\n",
        "            \"Unsupervised Learning\",\n",
        "            \"التعلم تحت الإشراف\",\n",
        "            \"التعلم غير المنظم\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"ما هو الفرق بين السببية والارتباط؟\",\n",
        "        \"answer\": (\n",
        "            \"السببية تشير إلى العلاقة التي تؤدي فيها التغييرات في أحد المتغيرات إلى تغييرات في متغير آخر، حيث تكون هناك علاقة بين السبب والنتيجة.\\n\"\n",
        "            \"* على سبيل المثال، يمكن أن يكون هناك علاقة سببية بين التدخين والإصابة بسرطان الرئة، حيث يؤدي التدخين إلى زيادة خطر الإصابة بسرطان الرئة.\\n\"\n",
        "            \"أما الارتباط فيشير إلى العلاقة بين متغيرين أو أكثر من حيث كيفية تحركهم معًا، ولكن لا يعني بالضرورة أن أحدهم يسبب الآخر.\\n\"\n",
        "            \"* على سبيل المثال، يمكن أن يكون هناك ارتباط بين درجات الحرارة العالية وزيادة في استهلاك المثلجات، لكن هذا لا يعني أن زيادة الحرارة هي السبب في زيادة الاستهلاك.\"\n",
        "        ),\n",
        "        \"domain_terms\": [\n",
        "            \"السببية\",\n",
        "            \"الارتباط\",\n",
        "            \"العلاقة السببية\",\n",
        "            \"الارتباط بين المتغيرات\"\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "generated_answers_mistral7b = [\"\"\"\n",
        "    \"\n",
        "نماذج التدريب المستقبلية (Supervised Learning) : هي نوع من النماذج التي تتطلب معالجة بيانات مُحددة أو بيانات مُعلقة لتكون قادرة على تعلم خطوات تصحيح أو تحديد الخاصية.\n",
        "نماذج التدريب الغير مستقبلية (Unsupervised Learning) : هي نوع من النماذج التي تتطلب معالجة بيانات غير مُحددة أو بيانات مُجمدة، لكي تكون قادرة على التعلم بشكل خاص في معرفة خاصية جديدة أو فرز البيانات في مجموعات غير مُحددة.\n",
        "نماذج التدريب السياسية (Semi-Supervised Learning) : هي نوع من النماذج التي تتطلب معالجة بيانات مُحددة وغير مُحددة، لكي تكون قادرة على التعلم بشكل خاص في معرفة خاصية جديدة.\n",
        "نماذج التدريب المستمر (Reinforcement Learning) : هي نوع من النماذج التي تتطلب معالجة بيانات غير مُحددة أو مُحددة، لكي تكون قادرة على التعلم بشكل خاص في إجراء الاختيارات المميزة في مستوى أعلى. \"\"\"\" \",\n",
        "\n",
        "    \"\"\"أحد الفرق الرئيسي الرئيسي بين التعلم تحت الإشراف (Supervised Learning) وغير المنظم (Unsupervised Learning) هو عدم وجود مثال معلم (Labeled Data) في الخطوة الثانية.\n",
        "\n",
        "1. التعلم المنظم (Supervised Learning): يتم تدريب النموذج على مجموعة من البيانات المُعلمة (Labeled Data), حيث يكون هناك أسئلة والتي يتم عرضها للنموذج ويتم عرض إجابة صحيحة (Correct Answer) لها. بعد ذلك، يتم استخدام النموذج المُدرب للتعليم على مجموعة من البيانات التي لم تتم عرضها للنموذج أثناء التدريب (Test Data) ويتم قياس النجاح الرأسي (Accuracy) للنموذج.\n",
        "2. التعلم الغير منظم (Unsupervised Learning): لا يوجد هناك إجابة صحيحة لأسئلة النموذج. النموذج يتعلم بالنفس من البيانات، ويقوم بفرزها إلى فئات أو تحديث خصائصها.\n",
        "\n",
        "كلما زادت عدم الإشراف على البيانات، كلما زادت الصعوبة في التعليق على مشاكل حقيقية. بالطبيعة، إذا كانت المشكلة غير مُعرفة أو لا يوجد بيانات مُعلمة لها، فهناك حاجة إلى التعلم الغير منظم. في الخطوة الثانية، يتم استخدام النموذج المُدرب للتعليق على مشاكل حقيقية غير مُعرفة.\"\"\",\n",
        "                     \"\"\"\n",
        "                     نعم، إن السببية والارتباط محدودتان للأسئلة في التعلم الآلي.\n",
        "\n",
        "* **السببية** (features) هي المقاييس التي يتم تحديدها للمشكلة ويتم استخدامها في عملية التحليل. مثلًا، إذا كنت تريد تحليل أنواع سبائك جديدة، فتحدد مقاييسها المتعددة مثل الحجم النوعي، والرصيف، والمعدن الخام.\n",
        "* **الارتباط** (relationship) هو علاقة بين السببية والإرجاع (output) مثل علاقة الحجم النوعي بالسبائك والتصنيع.\n",
        "\n",
        "في التعلم الآلي، نحدد السببية ونحللها بأشكال مختلفة مثل الإحصاء الأساسي (regression) والإقرار (classification). على الإطلاق، يؤدي العمل على السببية إلى معرفة علاقتها بالإرجاع.\n",
        "\"\"\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ArabicLightStemmer:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the Arabic Light Stemmer with prefix and suffix patterns\n",
        "        \"\"\"\n",
        "        # Common prefixes (ال, لل, بال, كال, فال, لل)\n",
        "        self.prefixes = [\n",
        "            'ال', 'لل', 'بال', 'كال', 'فال', 'لل', 'وال'\n",
        "        ]\n",
        "\n",
        "        # Common suffixes (ة, ه, ية, ات, ين, ون, ان, تي, تن, كم, هن, نا, يه, ته, تم)\n",
        "        self.suffixes = [\n",
        "            'ة', 'ه', 'ية', 'ات', 'ين', 'ون', 'ان', 'تي', 'تن',\n",
        "            'كم', 'هن', 'نا', 'يه', 'ته', 'تم', 'كن', 'ني', 'وا', 'ما', 'تا'\n",
        "        ]\n",
        "\n",
        "        # Length-three patterns\n",
        "        self.patterns = {\n",
        "            'فاع': 'فعل',\n",
        "            'فعل': 'فعل',\n",
        "            'فعي': 'فعل',\n",
        "            'فاعل': 'فعل',\n",
        "            'مفعل': 'فعل',\n",
        "            'تفعل': 'فعل',\n",
        "            'مفعول': 'فعل',\n",
        "            'يفعل': 'فعل',\n",
        "            'تفاعل': 'فعل',\n",
        "            'مفاعل': 'فعل',\n",
        "            'افتعل': 'فعل',\n",
        "            'افعال': 'فعل',\n",
        "            'فعال': 'فعل'\n",
        "        }\n",
        "\n",
        "    def remove_diacritics(self, word: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove Arabic diacritical marks from the word\n",
        "\n",
        "        Args:\n",
        "            word: Arabic word\n",
        "\n",
        "        Returns:\n",
        "            Word without diacritical marks\n",
        "        \"\"\"\n",
        "        return re.sub(r'[\\u064B-\\u065F\\u0640]', '', word)\n",
        "\n",
        "    def normalize_hamza(self, word: str) -> str:\n",
        "        \"\"\"\n",
        "        Normalize different forms of hamza to a single form\n",
        "\n",
        "        Args:\n",
        "            word: Arabic word\n",
        "\n",
        "        Returns:\n",
        "            Word with normalized hamza\n",
        "        \"\"\"\n",
        "        # Convert hamza forms to alef\n",
        "        hamza_forms = ['أ', 'إ', 'آ']\n",
        "        for hamza in hamza_forms:\n",
        "            word = word.replace(hamza, 'ا')\n",
        "        return word\n",
        "\n",
        "    def remove_prefix(self, word: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove common Arabic prefixes from the word\n",
        "\n",
        "        Args:\n",
        "            word: Arabic word\n",
        "\n",
        "        Returns:\n",
        "            Word with prefix removed if found\n",
        "        \"\"\"\n",
        "        if len(word) < 4:  # Don't process words that are too short\n",
        "            return word\n",
        "\n",
        "        for prefix in sorted(self.prefixes, key=len, reverse=True):\n",
        "            if word.startswith(prefix) and len(word) > len(prefix) + 2:\n",
        "                return word[len(prefix):]\n",
        "        return word\n",
        "\n",
        "    def remove_suffix(self, word: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove common Arabic suffixes from the word\n",
        "\n",
        "        Args:\n",
        "            word: Arabic word\n",
        "\n",
        "        Returns:\n",
        "            Word with suffix removed if found\n",
        "        \"\"\"\n",
        "        if len(word) < 4:  # Don't process words that are too short\n",
        "            return word\n",
        "\n",
        "        for suffix in sorted(self.suffixes, key=len, reverse=True):\n",
        "            if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
        "                return word[:-len(suffix)]\n",
        "        return word\n",
        "\n",
        "    def normalize_pattern(self, word: str) -> str:\n",
        "        \"\"\"\n",
        "        Normalize word pattern to its root form if possible\n",
        "\n",
        "        Args:\n",
        "            word: Arabic word\n",
        "\n",
        "        Returns:\n",
        "            Word with normalized pattern\n",
        "        \"\"\"\n",
        "        if len(word) < 3:\n",
        "            return word\n",
        "\n",
        "        # Extract the pattern for length-three words\n",
        "        if len(word) == 3:\n",
        "            return word\n",
        "\n",
        "        # Check for known patterns\n",
        "        for pattern, replacement in self.patterns.items():\n",
        "            if len(word) == len(pattern) and self._matches_pattern(word, pattern):\n",
        "                return replacement\n",
        "\n",
        "        return word\n",
        "\n",
        "    def _matches_pattern(self, word: str, pattern: str) -> bool:\n",
        "        \"\"\"\n",
        "        Check if a word matches a given pattern\n",
        "\n",
        "        Args:\n",
        "            word: Arabic word\n",
        "            pattern: Pattern to match against\n",
        "\n",
        "        Returns:\n",
        "            True if word matches pattern, False otherwise\n",
        "        \"\"\"\n",
        "        if len(word) != len(pattern):\n",
        "            return False\n",
        "\n",
        "        # Extract root letters based on pattern\n",
        "        root_positions = [i for i, char in enumerate(pattern) if char in 'فعل']\n",
        "        non_root_positions = [i for i, char in enumerate(pattern) if char not in 'فعل']\n",
        "\n",
        "        # Check if non-root letters match the pattern\n",
        "        for pos in non_root_positions:\n",
        "            if word[pos] != pattern[pos]:\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def stem(self, word: str) -> str:\n",
        "        \"\"\"\n",
        "        Apply light stemming to an Arabic word\n",
        "\n",
        "        Args:\n",
        "            word: Arabic word to stem\n",
        "\n",
        "        Returns:\n",
        "            Stemmed word\n",
        "        \"\"\"\n",
        "        if not word:\n",
        "            return word\n",
        "\n",
        "        # Preprocessing\n",
        "        word = self.remove_diacritics(word)\n",
        "        word = self.normalize_hamza(word)\n",
        "\n",
        "        # Remove affixes\n",
        "        word = self.remove_prefix(word)\n",
        "        word = self.remove_suffix(word)\n",
        "\n",
        "        # Normalize patterns\n",
        "        word = self.normalize_pattern(word)\n",
        "\n",
        "        return word\n",
        "\n",
        "    def stem_text(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Apply stemming to all words in a text\n",
        "\n",
        "        Args:\n",
        "            text: Arabic text\n",
        "\n",
        "        Returns:\n",
        "            List of stemmed words\n",
        "        \"\"\"\n",
        "        # Split text into words (simple tokenization)\n",
        "        words = text.split()\n",
        "\n",
        "        # Apply stemming to each word\n",
        "        return [self.stem(word) for word in words]"
      ],
      "metadata": {
        "id": "A6yOUXi3mgrc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArabicTokenizer:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the Arabic tokenizer with necessary patterns and rules\n",
        "        \"\"\"\n",
        "        # Arabic punctuation marks\n",
        "        self.ar_punctuation = set('،؛؟٪'\"«»…٬٫'\")\n",
        "\n",
        "        # Additional punctuation that might appear in Arabic text\n",
        "        self.en_punctuation = set(',.!?;:\\'\"`()[]{}<>/-+*=\\\\|~@#$%^&')\n",
        "\n",
        "        # Arabic numbers\n",
        "        self.ar_numbers = set('٠١٢٣٤٥٦٧٨٩')\n",
        "\n",
        "        # Compile regex patterns\n",
        "        self.patterns = {\n",
        "            'tatweel': re.compile(r'\\u0640+'),  # Tatweel character\n",
        "            'spacing': re.compile(r'\\s+'),  # Multiple spaces\n",
        "            'digits': re.compile(r'[\\u0660-\\u0669\\u06F0-\\u06F9\\d]+'),  # Arabic and English digits\n",
        "            'symbols': re.compile(r'[﷼﷽٪‰\\u0660-\\u0669]'),  # Special symbols\n",
        "            'english': re.compile(r'[a-zA-Z]+[a-zA-Z\\d]*'),  # English words\n",
        "            'urls': re.compile(r'https?://\\S+|www\\.\\S+'),  # URLs\n",
        "            'emails': re.compile(r'\\S+@\\S+\\.\\S+'),  # Email addresses\n",
        "            'mentions': re.compile(r'@\\w+'),  # Social media mentions\n",
        "            'hashtags': re.compile(r'#\\w+'),  # Hashtags\n",
        "        }\n",
        "\n",
        "    def is_arabic_char(self, char: str) -> bool:\n",
        "        \"\"\"\n",
        "        Check if a character is an Arabic letter\n",
        "\n",
        "        Args:\n",
        "            char: Single character to check\n",
        "\n",
        "        Returns:\n",
        "            True if the character is an Arabic letter\n",
        "        \"\"\"\n",
        "        return '\\u0600' <= char <= '\\u06FF' or '\\u0750' <= char <= '\\u077F'\n",
        "\n",
        "    def remove_tatweel(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove tatweel (kashida) from text\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "\n",
        "        Returns:\n",
        "            Text with tatweel removed\n",
        "        \"\"\"\n",
        "        return self.patterns['tatweel'].sub('', text)\n",
        "\n",
        "    def normalize_spaces(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Normalize multiple spaces to single space\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "\n",
        "        Returns:\n",
        "            Text with normalized spaces\n",
        "        \"\"\"\n",
        "        return self.patterns['spacing'].sub(' ', text)\n",
        "\n",
        "    def handle_special_tokens(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extract special tokens (URLs, emails, mentions, hashtags)\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "\n",
        "        Returns:\n",
        "            List of special tokens found\n",
        "        \"\"\"\n",
        "        special_tokens = []\n",
        "\n",
        "        # Find and collect special tokens\n",
        "        for pattern_name in ['urls', 'emails', 'mentions', 'hashtags']:\n",
        "            matches = self.patterns[pattern_name].finditer(text)\n",
        "            special_tokens.extend(match.group() for match in matches)\n",
        "\n",
        "        return special_tokens\n",
        "\n",
        "    def tokenize(self, text: str, preserve_special: bool = True) -> List[str]:\n",
        "        \"\"\"\n",
        "        Tokenize Arabic text\n",
        "\n",
        "        Args:\n",
        "            text: Input Arabic text\n",
        "            preserve_special: Whether to preserve special tokens (URLs, emails, etc.)\n",
        "\n",
        "        Returns:\n",
        "            List of tokens\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        # Initial preprocessing\n",
        "        text = self.normalize_spaces(text)\n",
        "        text = self.remove_tatweel(text)\n",
        "\n",
        "        # Handle special tokens\n",
        "        special_tokens = []\n",
        "        if preserve_special:\n",
        "            special_tokens = self.handle_special_tokens(text)\n",
        "            # Replace special tokens with placeholders\n",
        "            for token in special_tokens:\n",
        "                text = text.replace(token, ' __SPECIAL__ ')\n",
        "\n",
        "        tokens = []\n",
        "        current_token = []\n",
        "\n",
        "        for char in text:\n",
        "            # Skip placeholders (will be replaced later)\n",
        "            if char == '_':\n",
        "                continue\n",
        "\n",
        "            # Handle whitespace\n",
        "            if char.isspace():\n",
        "                if current_token:\n",
        "                    tokens.append(''.join(current_token))\n",
        "                    current_token = []\n",
        "                continue\n",
        "\n",
        "            # Handle Arabic letters\n",
        "            if self.is_arabic_char(char):\n",
        "                current_token.append(char)\n",
        "                continue\n",
        "\n",
        "            # Handle numbers (both Arabic and English)\n",
        "            if char.isdigit() or char in self.ar_numbers:\n",
        "                if current_token and not (current_token[-1].isdigit() or current_token[-1] in self.ar_numbers):\n",
        "                    tokens.append(''.join(current_token))\n",
        "                    current_token = []\n",
        "                current_token.append(char)\n",
        "                continue\n",
        "\n",
        "            # Handle punctuation\n",
        "            if char in self.ar_punctuation or char in self.en_punctuation:\n",
        "                if current_token:\n",
        "                    tokens.append(''.join(current_token))\n",
        "                    current_token = []\n",
        "                tokens.append(char)\n",
        "                continue\n",
        "\n",
        "            # Handle other characters\n",
        "            if current_token:\n",
        "                tokens.append(''.join(current_token))\n",
        "                current_token = []\n",
        "            tokens.append(char)\n",
        "\n",
        "        # Add final token if exists\n",
        "        if current_token:\n",
        "            tokens.append(''.join(current_token))\n",
        "\n",
        "        # Restore special tokens\n",
        "        if preserve_special:\n",
        "            tokens = [token if token != '__SPECIAL__' else special_tokens.pop(0)\n",
        "                     for token in tokens if token != ' ']\n",
        "\n",
        "        return [token for token in tokens if token.strip()]\n",
        "\n",
        "    def tokenize_text(self, text: str, preserve_special: bool = True) -> List[str]:\n",
        "        \"\"\"\n",
        "        Tokenize a complete text, handling multiple sentences\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "            preserve_special: Whether to preserve special tokens\n",
        "\n",
        "        Returns:\n",
        "            List of tokens\n",
        "        \"\"\"\n",
        "        # Split text into sentences (basic implementation)\n",
        "        sentences = re.split(r'[.!?؟।\\n]+', text)\n",
        "\n",
        "        # Tokenize each sentence\n",
        "        all_tokens = []\n",
        "        for sentence in sentences:\n",
        "            if sentence.strip():\n",
        "                tokens = self.tokenize(sentence, preserve_special)\n",
        "                all_tokens.extend(tokens)\n",
        "\n",
        "        return all_tokens\n",
        "\n",
        "    @staticmethod\n",
        "    def detokenize(tokens: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Combine tokens back into text\n",
        "\n",
        "        Args:\n",
        "            tokens: List of tokens\n",
        "\n",
        "        Returns:\n",
        "            Combined text\n",
        "        \"\"\"\n",
        "        text = ''\n",
        "        for i, token in enumerate(tokens):\n",
        "            # Don't add space before punctuation\n",
        "            if i > 0 and not (token in '،؛؟.!?:;' or token.startswith('#') or token.startswith('@')):\n",
        "                text += ' '\n",
        "            text += token\n",
        "        return text.strip()\n",
        "\n",
        "def arabic_tokenize(text: str, preserve_special: bool = True) -> List[str]:\n",
        "    \"\"\"\n",
        "    Convenience function for tokenizing Arabic text\n",
        "\n",
        "    Args:\n",
        "        text: Input Arabic text\n",
        "        preserve_special: Whether to preserve special tokens\n",
        "\n",
        "    Returns:\n",
        "        List of tokens\n",
        "    \"\"\"\n",
        "    tokenizer = ArabicTokenizer()\n",
        "    return tokenizer.tokenize_text(text, preserve_special)"
      ],
      "metadata": {
        "id": "tvJ-eqR7oWZR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class ArabicTextEvaluator:\n",
        "    def __init__(self, model_name: str = 'bert-base-multilingual-cased'):\n",
        "        \"\"\"\n",
        "        Initialize with a multilingual model more suitable for Arabic\n",
        "        \"\"\"\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        self.smoothing = SmoothingFunction().method4\n",
        "        self.arabic_stemmer = ArabicLightStemmer()\n",
        "        self.tokenizer = ArabicTokenizer()  # Initialize the tokenizer\n",
        "\n",
        "    def preprocess_arabic_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Enhanced Arabic text preprocessing\n",
        "        \"\"\"\n",
        "        if not text or not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Normalize Unicode representations\n",
        "        text = normalize_unicode(text)\n",
        "\n",
        "        # Remove diacritics, tatweel, and kashida\n",
        "        text = re.sub(r'[\\u064B-\\u065F\\u0640]', '', text)\n",
        "\n",
        "        # Normalize Arabic characters\n",
        "        replacements = {\n",
        "            'أ': 'ا', 'إ': 'ا', 'آ': 'ا',  # Alef forms\n",
        "            'ى': 'ي', 'ئ': 'ي',  # Yaa forms\n",
        "            'ة': 'ه',  # Taa marbouta\n",
        "            '٠': '0', '١': '1', '٢': '2', '٣': '3', '٤': '4',  # Arabic numerals\n",
        "            '٥': '5', '٦': '6', '٧': '7', '٨': '8', '٩': '9'\n",
        "        }\n",
        "        for old, new in replacements.items():\n",
        "            text = text.replace(old, new)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def tokenize_text(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Tokenize Arabic text using ArabicTokenizer\n",
        "\n",
        "        Args:\n",
        "            text: Input Arabic text\n",
        "\n",
        "        Returns:\n",
        "            List of tokens\n",
        "        \"\"\"\n",
        "        return self.tokenizer.tokenize_text(text)\n",
        "\n",
        "    def compute_fluency_score(self, text: str) -> float:\n",
        "        \"\"\"\n",
        "        Compute Arabic text fluency score\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return 0.0\n",
        "\n",
        "        # Basic fluency metrics using the tokenizer\n",
        "        words = self.tokenize_text(text)\n",
        "        if not words:\n",
        "            return 0.0\n",
        "\n",
        "        # Check for basic Arabic sentence structure\n",
        "        has_verb = any(self.is_arabic_verb(word) for word in words)\n",
        "        has_noun = any(self.is_arabic_noun(word) for word in words)\n",
        "        proper_length = 3 <= len(words) <= 50  # Reasonable sentence length\n",
        "\n",
        "        # Combine metrics\n",
        "        fluency_score = (has_verb + has_noun + proper_length) / 3\n",
        "        return fluency_score\n",
        "\n",
        "    def compute_semantic_coherence(self, generated: str, reference: str) -> float:\n",
        "        \"\"\"\n",
        "        Compute semantic coherence between generated and reference texts.\n",
        "        \"\"\"\n",
        "        gen_emb = self.embedding_model.encode([generated], convert_to_tensor=True)\n",
        "        ref_emb = self.embedding_model.encode([reference], convert_to_tensor=True)\n",
        "        return float(cosine_similarity(gen_emb, ref_emb)[0][0])\n",
        "\n",
        "    def compute_domain_coverage(\n",
        "        self,\n",
        "        text: str,\n",
        "        domain_terms: List[str],\n",
        "        weights: Dict[str, float] = None\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Improved domain coverage with stemming and proper tokenization\n",
        "        \"\"\"\n",
        "        if not text or not domain_terms:\n",
        "            return 0.0\n",
        "\n",
        "        if weights is None:\n",
        "            weights = {term: 1.0 for term in domain_terms}\n",
        "\n",
        "        # Preprocess and stem text using proper tokenization\n",
        "        text_tokens = set(self.tokenize_text(text))\n",
        "        text_stems = {self.arabic_stemmer.stem(token) for token in text_tokens}\n",
        "\n",
        "        # Stem domain terms\n",
        "        domain_stems = {self.arabic_stemmer.stem(term): term for term in domain_terms}\n",
        "\n",
        "        # Calculate coverage with stemming\n",
        "        covered_terms = set()\n",
        "        for text_stem in text_stems:\n",
        "            if text_stem in domain_stems:\n",
        "                covered_terms.add(domain_stems[text_stem])\n",
        "\n",
        "        weighted_coverage = sum(weights.get(term, 1.0) for term in covered_terms)\n",
        "        total_weight = sum(weights.values())\n",
        "\n",
        "        return weighted_coverage / total_weight if total_weight > 0 else 0.0\n",
        "\n",
        "    def compute_enhanced_wssa(\n",
        "        self,\n",
        "        generated_answers: List[str],\n",
        "        reference_answers: List[str],\n",
        "        domain_terms: List[List[str]],\n",
        "        weights: Dict[str, float] = None\n",
        "    ) -> List[Dict[str, float]]:\n",
        "        \"\"\"\n",
        "        Improved WSSA implementation with proper tokenization\n",
        "        \"\"\"\n",
        "        if weights is None:\n",
        "            weights = {\n",
        "                'semantic': 0.35,\n",
        "                'domain': 0.25,\n",
        "                'fluency': 0.15,\n",
        "                'bleu': 0.15,\n",
        "                'rouge': 0.10\n",
        "            }\n",
        "\n",
        "        # Validate weights\n",
        "        if not math.isclose(sum(weights.values()), 1.0, rel_tol=1e-9):\n",
        "            raise ValueError(\"Weights must sum to 1.0\")\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for gen_ans, ref_ans, terms in zip(generated_answers, reference_answers, domain_terms):\n",
        "            # Preprocess texts\n",
        "            gen_ans_proc = self.preprocess_arabic_text(gen_ans)\n",
        "            ref_ans_proc = self.preprocess_arabic_text(ref_ans)\n",
        "\n",
        "            if not gen_ans_proc or not ref_ans_proc:\n",
        "                continue\n",
        "\n",
        "            # Component scores\n",
        "            semantic_score = self.compute_semantic_coherence(gen_ans_proc, ref_ans_proc)\n",
        "            domain_score = self.compute_domain_coverage(gen_ans_proc, terms)\n",
        "            fluency_score = self.compute_fluency_score(gen_ans_proc)\n",
        "\n",
        "            # BLEU score with proper tokenization\n",
        "            bleu_score = sentence_bleu(\n",
        "                [self.tokenize_text(ref_ans_proc)],\n",
        "                self.tokenize_text(gen_ans_proc),\n",
        "                smoothing_function=self.smoothing\n",
        "            )\n",
        "\n",
        "            # ROUGE scores\n",
        "            rouge_scores = self.rouge_scorer.score(ref_ans_proc, gen_ans_proc)\n",
        "            rouge_l = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "            # Compute weighted final score\n",
        "            final_score = (\n",
        "                weights['semantic'] * semantic_score +\n",
        "                weights['domain'] * domain_score +\n",
        "                weights['fluency'] * fluency_score +\n",
        "                weights['bleu'] * bleu_score +\n",
        "                weights['rouge'] * rouge_l\n",
        "            )\n",
        "\n",
        "            metrics = {\n",
        "                'final_score': final_score,\n",
        "                'semantic_coherence': semantic_score,\n",
        "                'domain_coverage': domain_score,\n",
        "                'fluency': fluency_score,\n",
        "                'bleu': bleu_score,\n",
        "                'rouge_l': rouge_l\n",
        "            }\n",
        "\n",
        "            results.append(metrics)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def is_arabic_verb(self, word: str) -> bool:\n",
        "        \"\"\"\n",
        "        Simple check if a word is likely an Arabic verb\n",
        "        Note: This is a basic implementation and should be enhanced\n",
        "        \"\"\"\n",
        "        # Common verb patterns\n",
        "        verb_patterns = [\n",
        "            'فعل', 'يفعل', 'افعل', 'مفعول',\n",
        "            'فاعل', 'تفعل', 'انفعل', 'استفعل'\n",
        "        ]\n",
        "        stemmed_word = self.arabic_stemmer.stem(word)\n",
        "        return any(pattern in stemmed_word for pattern in verb_patterns)\n",
        "\n",
        "    def is_arabic_noun(self, word: str) -> bool:\n",
        "        \"\"\"\n",
        "        Simple check if a word is likely an Arabic noun\n",
        "        Note: This is a basic implementation and should be enhanced\n",
        "        \"\"\"\n",
        "        # Common noun patterns\n",
        "        noun_patterns = [\n",
        "            'فاعل', 'مفعول', 'فعال', 'فعيل',\n",
        "            'مفعل', 'مفعال', 'فعلان', 'فعول'\n",
        "        ]\n",
        "        stemmed_word = self.arabic_stemmer.stem(word)\n",
        "        return any(pattern in stemmed_word for pattern in noun_patterns)\n",
        "# Example usage\n",
        "correct_answers = [item[\"answer\"] for item in dataset]\n",
        "domain_terms = [item[\"domain_terms\"] for item in dataset]\n",
        "evaluator = ArabicTextEvaluator()\n",
        "\n",
        "# Compute results for both models\n",
        "results_llama = evaluator.compute_enhanced_wssa(generated_answers_llama, correct_answers, domain_terms)\n",
        "results_mistral7b = evaluator.compute_enhanced_wssa(generated_answers_mistral7b, correct_answers, domain_terms)\n",
        "\n",
        "# Display results for each model\n",
        "print(\"\\nResults for LLaMA:\")\n",
        "for i, metrics in enumerate(results_llama, 1):\n",
        "    print(f\"\\nQuestion {i} Evaluation Metrics:\")\n",
        "    for metric, score in metrics.items():\n",
        "        print(f\"{metric}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axPKs7tZ5NxF",
        "outputId": "9569c639-15f3-4bb9-81b2-b8d10407904e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-multilingual-cased. Creating a new one with mean pooling.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for LLaMA:\n",
            "\n",
            "Question 1 Evaluation Metrics:\n",
            "final_score: 0.3788\n",
            "semantic_coherence: 0.8449\n",
            "domain_coverage: 0.0000\n",
            "fluency: 0.3333\n",
            "bleu: 0.0205\n",
            "rouge_l: 0.3000\n",
            "\n",
            "Question 2 Evaluation Metrics:\n",
            "final_score: 0.3798\n",
            "semantic_coherence: 0.9058\n",
            "domain_coverage: 0.0000\n",
            "fluency: 0.3333\n",
            "bleu: 0.0854\n",
            "rouge_l: 0.0000\n",
            "\n",
            "Question 3 Evaluation Metrics:\n",
            "final_score: 0.5015\n",
            "semantic_coherence: 0.9201\n",
            "domain_coverage: 0.5000\n",
            "fluency: 0.3333\n",
            "bleu: 0.0295\n",
            "rouge_l: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nResults for Mistral7B:\")\n",
        "for i, metrics in enumerate(results_mistral7b, 1):\n",
        "    print(f\"\\nQuestion {i} Evaluation Metrics:\")\n",
        "    for metric, score in metrics.items():\n",
        "        print(f\"{metric}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vugp4RNr63Xc",
        "outputId": "cfca44e0-2b40-4df2-cdde-5805fd8975f4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for Mistral7B:\n",
            "\n",
            "Question 1 Evaluation Metrics:\n",
            "final_score: 0.4872\n",
            "semantic_coherence: 0.9173\n",
            "domain_coverage: 0.0000\n",
            "fluency: 0.3333\n",
            "bleu: 0.3303\n",
            "rouge_l: 0.6667\n",
            "\n",
            "Question 2 Evaluation Metrics:\n",
            "final_score: 0.4381\n",
            "semantic_coherence: 0.9310\n",
            "domain_coverage: 0.0000\n",
            "fluency: 0.3333\n",
            "bleu: 0.1829\n",
            "rouge_l: 0.3478\n",
            "\n",
            "Question 3 Evaluation Metrics:\n",
            "final_score: 0.4867\n",
            "semantic_coherence: 0.8870\n",
            "domain_coverage: 0.5000\n",
            "fluency: 0.3333\n",
            "bleu: 0.0086\n",
            "rouge_l: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compare results between models\n",
        "print(\"\\nComparison between LLaMA and Mistral7B:\")\n",
        "comparison_results = []\n",
        "for i, (llama_metrics, mistral_metrics) in enumerate(zip(results_llama, results_mistral7b), 1):\n",
        "    comparison = {\n",
        "        'question': i,\n",
        "        'better_model': 'LLaMA' if llama_metrics['final_score'] > mistral_metrics['final_score'] else 'Mistral7B',\n",
        "        'llama_final_score': llama_metrics['final_score'],\n",
        "        'mistral_final_score': mistral_metrics['final_score'],\n",
        "        'semantic_difference': llama_metrics['semantic_coherence'] - mistral_metrics['semantic_coherence'],\n",
        "        'domain_difference': llama_metrics['domain_coverage'] - mistral_metrics['domain_coverage'],\n",
        "        'bleu_difference': llama_metrics['bleu'] - mistral_metrics['bleu'],\n",
        "        'rouge_difference': llama_metrics['rouge_l'] - mistral_metrics['rouge_l']\n",
        "    }\n",
        "    comparison_results.append(comparison)\n",
        "\n",
        "    print(f\"\\nQuestion {i} Comparison:\")\n",
        "    print(f\"Better Model: {comparison['better_model']}\")\n",
        "    print(f\"LLaMA Final Score: {comparison['llama_final_score']:.4f}\")\n",
        "    print(f\"Mistral7B Final Score: {comparison['mistral_final_score']:.4f}\")\n",
        "    print(f\"Semantic Coherence Difference: {comparison['semantic_difference']:.4f}\")\n",
        "    print(f\"Domain Coverage Difference: {comparison['domain_difference']:.4f}\")\n",
        "    print(f\"BLEU Difference: {comparison['bleu_difference']:.4f}\")\n",
        "    print(f\"ROUGE-L Difference: {comparison['rouge_difference']:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rsQhZtQr7ggp",
        "outputId": "71755f42-f4ea-4b8f-c729-6b40394f738b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison between LLaMA and Mistral7B:\n",
            "\n",
            "Question 1 Comparison:\n",
            "Better Model: Mistral7B\n",
            "LLaMA Final Score: 0.3788\n",
            "Mistral7B Final Score: 0.4872\n",
            "Semantic Coherence Difference: -0.0724\n",
            "Domain Coverage Difference: 0.0000\n",
            "BLEU Difference: -0.3098\n",
            "ROUGE-L Difference: -0.3667\n",
            "\n",
            "Question 2 Comparison:\n",
            "Better Model: Mistral7B\n",
            "LLaMA Final Score: 0.3798\n",
            "Mistral7B Final Score: 0.4381\n",
            "Semantic Coherence Difference: -0.0253\n",
            "Domain Coverage Difference: 0.0000\n",
            "BLEU Difference: -0.0975\n",
            "ROUGE-L Difference: -0.3478\n",
            "\n",
            "Question 3 Comparison:\n",
            "Better Model: LLaMA\n",
            "LLaMA Final Score: 0.5015\n",
            "Mistral7B Final Score: 0.4867\n",
            "Semantic Coherence Difference: 0.0331\n",
            "Domain Coverage Difference: 0.0000\n",
            "BLEU Difference: 0.0210\n",
            "ROUGE-L Difference: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GLwT1TqY7imu"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}